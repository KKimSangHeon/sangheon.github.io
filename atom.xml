<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Kim Sang Heon&#39;s Bolg</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://KKimSangHeon.github.io/"/>
  <updated>2019-06-25T13:12:51.692Z</updated>
  <id>http://KKimSangHeon.github.io/</id>
  
  <author>
    <name>Kim Sang Heon</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2.TCP/IP와의 만남</title>
    <link href="http://KKimSangHeon.github.io/2019/06/25/network2/"/>
    <id>http://KKimSangHeon.github.io/2019/06/25/network2/</id>
    <published>2019-06-25T11:46:10.000Z</published>
    <updated>2019-06-25T13:12:51.692Z</updated>
    
    <content type="html"><![CDATA[<p>오늘날 가장 많이 사용되는 프로토콜은 TCP/IP 이다.(인터넷 때문..)<br>Transmission Control Protocol/Internet Protocol의 약자로서 ARPANET에 의해 처음 개발되었다. 각각의 네트워크에 접속되는 호스트는 다른 네트워크에 연결되어 있는 호스트까지도 서로 데이터를 주고받을 수 있다. 이 때 사용하는 호스트들의 고유 주소인 IP주소는 Internet Network Information Center(InterNIC)이란 단체에서 관리 분배되고 있다.</p>
<p>IP의 한정으로 인해 내부 네트워크에서는 공인되지 않은 IP주소를 사용하고 인터넷으로 나갈 때만 공인주소(유일한 IPwnth)를 가지고 나가는 방식인 NAT(Network Address Translation)나 동일한 IP 주소를 가지고 여러 명이 인터넷에 접속하면서 포트 넘버만을 바꾸는 PAT 등이 사용되고 있다.</p>
<p>IP주소는 2진수 32개로 만들어졌다. 즉 만들 수 있는 주소는 2^32만큼이다. 이를 IPv4라 하고 고갈 대책으로 나온것이 IPv6인데 2^128개의 주소를 표현할 수 있다.</p>
<p><code>DHCP</code><br>IP주소를 자동으로 배정해준다. DHCP 서버가 따로 존재하며 DHCP클라이언트들이 주소를 요청하면 DHCP 서버는 갖고있는 주소중에서 하나를 자동으로 할당해준다.</p>
<p><code>NIC(Network Interface Card) 즉 랜카드</code><br>유저의 데이터를 케이블에실어서 허브나 스위치 혹은 라우터 등으로 전달해주고 자신에게 온 데이터를 CPU에게 전달해주는 역할<br>대부분(90% 이상) 이더넷용 랜카드이다. UTP타입을 많이쓴다고만 알아두자.</p>
<p><code>허브</code><br>직사각형 상자에 구멍이 뚫려있는 모양으로 구멍에 따라 몇 포트 허브다라고 한다. 구멍의 숫자가 몇대의 장비를 연결할수 있는지 결정한다. 또한 같은 허브에 연결된 PC끼리는 서로 통신이 가능하다. 허브는 멀티포트 리피터라고 말 할 수 있다. 즉 포트가 많고 들어온 데이터를 그대로 재전송한다는것. 쉽게말하면 한 포트로 들어온 데이터를 나머지 모든 포트로 뿌려준다는 것(이 때 받은 데이터를 수신한 랜카드들은 목적지 맥 어드레스를 보고 버릴지 CPU로 올릴지 판단한다.). 한 허브에 연결된 모든 PC는 같은 콜리전 도메인 안에 있다.</p>
<p><code>리피터</code><br>두 장비를 UTP로 통신할 때 100m 이상인경우 권장되지 않는다. (완전한 통신보장x) 이 경우 중간에 리피터를 두어 들어온 데이터를 다른 쪽으로 전달해 주는 역할을 한다. 허브가 리피터의 역할을 대신하게 되어 만나기가 쉽지 않다.</p>
<p><code>허브의 한계</code><br>허브의 수를 늘리기 위해 다른 허브에 물리게 된다면 collistion domain이 커지게 된다. 이로 인해 콜리전이 자주 발생하게 되는 결과를 낳는다.</p>
<h3 id="허브의-종류"><a href="#허브의-종류" class="headerlink" title="허브의 종류"></a>허브의 종류</h3><p>허브는 보통 인텔리전트 허브, 더미허브, SemiIntelligent 허브로 나뉜다.</p>
<p><code>인텔리전트 허브</code> : NMS(네트워크 관리시스템)를 통해서 관리가 되는가로 인텔리전트 허브와 더미 허브로 나눈다. 즉 인텔리전트 허브는 NMS상에서 모든 데이터를 분석, 제어가 가능하다. 특정 PC가 자꾸 데이터를 보내 컬리전이 발생킨다면 네트워크에서 분리시킴으로써 해결할 수있는 기능도 갖고있다. 또한 분리된 포트는 허브에서 램프로 표시되기 때문에 바로 조치가 가능하고 이를 Auto Partition이라고 한다.</p>
<p><code>Semi 더미 허브</code> : 더미 허브이지만 인텔리전트 허브와 연결하면 자기도 인텔리전트 허브가 된다. 즉 혼자 있을때는 더미허브, 인텔리전트 허브랑 같이 있으면 인텔리전트 허브가 된다.</p>
<p><code>Stackable 허브</code>: 스택이 가능한 허브, 즉 쌓을 수 있는 허브, 학교 전산실에 허브 여러개 쌓아둔거 생각해보자.</p>
<h3 id="허브의-끝-스위치의-시작-허브보다-두-수-높은-스위치"><a href="#허브의-끝-스위치의-시작-허브보다-두-수-높은-스위치" class="headerlink" title="허브의 끝 스위치의 시작(허브보다 두 수 높은 스위치!)"></a>허브의 끝 스위치의 시작(허브보다 두 수 높은 스위치!)</h3><p><code>콜리전 도메인을 낮춘 브리지 또는 스위치!</code><br>스위치가 나오기 전까지는 브릿지가 다 해주었지만 이제 브릿지보다 빠른 스위치가 나와 브릿지는 물러나고 있다.<br>스위치는 1번 포트에 연결된 PC가 2번 포트에 연결된 PC와 데이터를 주고받는 동안에도 3번 포트에 연결된 PC와 4번 포트에 연결된 PC가 서로 데이터를 주고 받을수 있게 해주는 장비.<br>유식하게 포트별로 콜리전 도메인이 나뉘어져 있다고 한다.<br>허브는 한 순간에 한대의 차만달릴 수 있다면 스위치는 포트 수별로 차선이 존재하여 여러대가 달릴 수 있는구조이다. 그렇지만 스위치 또한 서버와의 통신은 한순간에 하나의 PC만이 가능하다.</p>
<h3 id="허브보다-한-수-높은-브릿지"><a href="#허브보다-한-수-높은-브릿지" class="headerlink" title="허브보다 한 수 높은 브릿지"></a>허브보다 한 수 높은 브릿지</h3><p>말그대로 다리이다. 스위치와 사촌지간이며 하는일이 서로 비슷하다.<br>브릿지는 허브로 만들어진 콜리전 도메인 사이를 반으로 나누고 중간에 다리를 놓는다. 이로 인해 다리 남단은 남단끼리 북단은 북단끼리 동시에 통신이 가능해진다.<br>정리하자면 브릿지는 허브보다 한 수 위 장비인데 콜리전 도메인을 나누어 준다.(중간에 브릿지를 만들어서)</p>
<h3 id="브릿지-스위치의-기능"><a href="#브릿지-스위치의-기능" class="headerlink" title="브릿지/스위치의 기능"></a>브릿지/스위치의 기능</h3><ul>
<li>Learning. 배운다</li>
<li>Flooding. 모르면 들어온 포트를 제외한 다른 모든 포트로 뿌린다.</li>
<li>Forwarding. 해당 포트로 건네준다.</li>
<li>Filtering. 다른 포트로는 못건너가게 막는다</li>
<li>Aging. 나이를 먹는다.</li>
</ul>
<p>/</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;오늘날 가장 많이 사용되는 프로토콜은 TCP/IP 이다.(인터넷 때문..)&lt;br&gt;Transmission Control Protocol/Internet Protocol의 약자로서 ARPANET에 의해 처음 개발되었다. 각각의 네트워크에 접속되는 호
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="Network" scheme="http://KKimSangHeon.github.io/categories/CS/Network/"/>
    
    
  </entry>
  
  <entry>
    <title>16. 장애대책</title>
    <link href="http://KKimSangHeon.github.io/2019/06/23/kube20/"/>
    <id>http://KKimSangHeon.github.io/2019/06/23/kube20/</id>
    <published>2019-06-23T08:30:27.000Z</published>
    <updated>2019-06-23T10:21:05.006Z</updated>
    
    <content type="html"><![CDATA[<h3 id="도커-운영-시의-장애-대책"><a href="#도커-운영-시의-장애-대책" class="headerlink" title="도커 운영 시의 장애 대책"></a>도커 운영 시의 장애 대책</h3><p>주로 dockerd 자체보다는 운영하는 사람의 실수나 부주의에서 비롯되거나 서버 리소스에 의한것이많다.</p>
<p><code>장애를 막기 위한 이미지 운영</code><br>의도하지 않은 컨테이너가 실행되는 경우</p>
<ul>
<li>운영 환경에서 latest버전의 이미지를 실행하거나 컨테이너 오케스트레이션 과정에서 최신 이미지를 실행한 상황</li>
<li>latest 외의 버전 이미지를 덮어쓴 상황</li>
<li>example/aaa:latest 태그로 빌드해야될 이미지를 example/bbb:latest 태그로 빌드한 상황(다른 이미지와 바뀜)</li>
</ul>
<p><code>이미지 테스트</code><br>실행단계에서 이미지가 잘못되었는지 판단하면 늦다. 이전에 이미지 테스트를 거치는것이 좋은데 이를 위해 container-structure-test가 많이 사용된다.<br> container-structure-test는 구글에서 오픈 소스로 공개한 테스트 프레임워크로 도커이미지를 테스트 대상으로 한다. 컨테이너안에 특정 파일의 존재여부, 파일 내용확인등을 진행할 수 있다.</p>
<p> <code>디스크 용량 부족</code><br> 도커 호스트 역시 디스크 용량 부족을 주의해야 한다. 용량이 가득차면 새로운 컨테이너를 만들수도 없고 기존 컨테이너의 실행에도 지장이 생긴다. 서드파티 모니터링 도구를 사용한 호스트 디스크 용량 모니터링은 필수이며 도커에서도 디스크 용량을 낭비하지 않도록 해야한다. 이를 위해 불필요한 이미지나 컨테이너는 디스크에서 삭제하는것이 좋다. 이럴경우 docker prune 명령어(사용하지 않는 이미지나 컨테이너 일괄 삭제)를 cron을 통해 야간에 실행하면 좋다</p>
<h3 id="쿠버네티스-운영-시의-장애-대책"><a href="#쿠버네티스-운영-시의-장애-대책" class="headerlink" title="쿠버네티스 운영 시의 장애 대책"></a>쿠버네티스 운영 시의 장애 대책</h3><p>쿠버네티스는 장애에 강한 컨테이너 오케스트레이션 시스템이지만, 노드 다운등 장애를 일으킬 수 있는 몇 가지 요인이 있다.</p>
<p><code>노드가 장애를 일으켰을 때 쿠버네티스의 동작?</code><br>노드가 장애를 일으켜 다운됐을 때 노드에 배포된 파드가 어떻게 되는지 알아봐야한다. 한 노드가 다운되면 정지된 파드들은 다른 노드로 배치된다. 이는 파드를 생성하는 레플리카 세트가 지정된 수의 파드를 유지하려고 하기 때문이다. 이를 오토힐링이라 한다. 레플리카세트가 관리하는 파드를 노드에서 의도적으로 삭제한 경우에도 같은 일이 일어난다. 즉 쿠버네티스에서는 레플리카세트를 관리하는 디플로이먼트나 스테이트풀세트, 데몬세트를 이용해 파드를 생성하는것이 가장 좋은 대책이다.</p>
<p><code>파드 안티 어피니티를 이용해 장애에 강한 파드 배치 전략 수립하기</code><br>레플리카세트의 오토힐링은 강력하지만 다른 노드로 파드가 재배치되는 동안에는 다운타임을 피할 수 없다.(replicas=1일때 치명적.)</p>
<p>그러므로 이러한 문제점의 피해를 최소화 하기위해 파드가 여러 노드에 나눠 배치되어야 한다. 이를 위해 replicas 값을 적절히 조절하는 방법을 사용한다. 파드가 여러 노드에 걸쳐 배치돼 있는 만큼 다운타임 없이 파드가 재배치 될 수 있을것이다.</p>
<p>하지만 파드 여러개가 한노드에 배치되어있는경우 해당 노드가 죽을경우 모두 정지되므로 파드가 여러개라도 다운타임이 생기게 된다. 쿠버네티스는 시스템 리소스가 여유 있는 노드를 골라 파드를 배치하기 때문에 앞서말한 가능성을 배제할 수 없다. 이를 해결하는 기능이 바로 <code>파드 안티 어피니티(pod antiaffinity)</code>이다. 이것은 파드간 상성을 고려한 배치전략을 규칙으로 정의한다. 이를 통해 C파드가 정의된 노드에는 D파드를 배치하지 말것과 같은 규칙을 정의할 수 있다. 디플로이먼트 정의에서 spec.affinity.podAntiAffinity설정에 이를 정의할 수 있다.<br>만약 replicas=3으로 했을 때 존재하는 노드가 2개 뿐이라면 하나의 파드는 펜딩상태가 되고 노드가 클러스터에 추가된 시점에 배치된다.</p>
<p>파드 어피니티라는 기능도 있는데 이는 파드 A는 파드 B와 자주 통신하므로 같은 노드에 배치한다 와 같은 경우에 활용할 수 있다.</p>
<p><code>CPU 부하가 큰 파드를 노드 어피니티로 격리하기</code><br>어플리케이션에 따라 CPU 부하가 큰 특성을 갖기도 한다. 배치잡처럼 CPU 부하가 클 경우 같은 노드 내 다른 파드의 성능을 떨어뜨린다. 이 경우 배치잡 파드를 전용 노드로 격리해야할 필요가 있다. 이를 위해 노드에 용도별로 구분짓는 레이블을 부여하고 파드 배치 규칙에 해당 레이블을 갖는 노드에만 파드를 배치하면 된다. <u>특정 레이블이 부여된 노드에만 파드를 배치하는 규칙을 정의하는 것이 노드 어피니티 이다.</u><br>노드에 레이블을 부여하기 위해 instancegroup 값을 설정하고 웹 어플리케이션이나 API파드만을 배치할 노드에는 webapi, 배치 잡만을 처리할 노드에는 batch라는 레이블을 붙여 용도를 구분한다. 정의된 규칙에 부합하는 노드가 없는경우 파드는 배치되지 않고 Pending 상태로 남는다.</p>
<p><code>HPA를 이용한 파드 오토 스케일링</code><br>HPA(Horizontal pod autoscaler)는 시스템 리소스 사용률에 따라 파드 수를 자동으로 조정하는 쿠버네티스 리소스이다. HPA는 파드의 오토 스케일링 조건을 디플로이먼트나 레플리카세트에 부여하기 위한 리소스다. 노드에 대한 파드의 CPU 사용률이 40퍼센트를 넘었을 때 파드에 오토 스케일링을 적용하고 싶을 때 사용하면 된다. 설정된 CPU 사용률 기준을 초과하면 자동으로 새로운 파드를 만들고 maxReplicas에 설정된 개수 이상의 파드는 생성하지 않는다.<br>HPA는 Cluster Autoscaler와 함께 사용할 때 효과가 극대화 된다.</p>
<p><code>Cluster Autoscaler를 이용한 노드 오토 스케일링</code><br>HPA가 파드의 오토 스케일링을 제공한다고 해도 파드를 배치할 노드 리소스가 충분하지 못할 수 있다. 이경우 Cluster Autoscaler을 사용한다. Cluster Autoscaler는 쿠버네티스 클러스터의 노드 수를 자동 조정한는 역할을 한다. 이는 쿠버네티스 리소스가 아니라 노드 오토 스케일링 기능을 제공하는 별도의 도구이다.</p>
<p><code>헬름의 릴리스 히스토리 제한</code><br>헬름으로 어플리케이션을 배포하는 경우 릴리즈 히스토리로 인한 문제가 발생할 수 있다. <code>kubectl -n kube-system get configmap</code>을 입력하면 어플리케이션명, 버전명이 붙은 컨피그 맵이 생성되어 있다. 헬름을 통해 설치, 업데이트를 반복하면 그만큼 컨피그 맵이 생기게 된다. 이를 피하기 위해 helm init 명령으로 틸러를 배포할 때 –history-max 옵션으로 히스토리 저장 최대 건수를 정한늑서이 좋다.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;도커-운영-시의-장애-대책&quot;&gt;&lt;a href=&quot;#도커-운영-시의-장애-대책&quot; class=&quot;headerlink&quot; title=&quot;도커 운영 시의 장애 대책&quot;&gt;&lt;/a&gt;도커 운영 시의 장애 대책&lt;/h3&gt;&lt;p&gt;주로 dockerd 자체보다는 운영하는 
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>15-1. 로그 관리방법(구글스택드라이버, stern)</title>
    <link href="http://KKimSangHeon.github.io/2019/06/23/kube19/"/>
    <id>http://KKimSangHeon.github.io/2019/06/23/kube19/</id>
    <published>2019-06-23T06:45:10.000Z</published>
    <updated>2019-06-23T08:30:01.300Z</updated>
    
    <content type="html"><![CDATA[<h3 id="구글-구글스택드라이버"><a href="#구글-구글스택드라이버" class="headerlink" title="구글 구글스택드라이버"></a>구글 구글스택드라이버</h3><p>구글 스택드라이버는 GCP나 AWS에서 로깅 및 모니터링에 사용되는 매니지드 서비스이다.</p>
<p><a href="https://app.google.stackdriver.com/" target="_blank" rel="external">https://app.google.stackdriver.com/</a> 으로 접속하거나 콘솔메뉴(왼쪽메뉴바)에서 모니터링을 선택하자.</p>
<p>그 후 Logging 탭을 선택하자.<br>스택드라이버에서는 로그를 Cloud HTTP 로드 밸런서나 GKE 컨테이너와 같은 카테고리 별로 나눠 볼 수 있다. 또한 컨테이너에서 JSON포맷 등으로 구조를 가진 로그를 출력한다면 <code>jsonPayload.속성:값</code> 형식으로 검색 결과를 좁힐 수 있다. 또한 입력 시에 가능한 속성명에 자동완성 기능을 제공해 더욱 편리하다. 이는 GKE 쿠버네티스 클러스터 노드에 배치된 fluentd-gcp리소스에서 제공한다.</p>
<p>개발자가 이를 관리할 필요가 없으며 컨테이너에서 로그를 json형태로 출력만 하면 로그를 확인할 수 있다. 스택드라이버는 어플리케이션 병목 검출(스택드라이버 트레이스), 어플리케이션 오류 탐지 기능(스택드라이버 에러 리포팅) 등의 기능을 제공한다. GKE에서는 필수적인 도구이다.</p>
<h3 id="stern"><a href="#stern" class="headerlink" title="stern"></a>stern</h3><p>키바나, 스택드라이버는 좋긴하지만 조금 거창한느낌이 있다. 이보다 가볍개 보는 방법으로 <code>kubectl logs -f 파드ID</code>라는 방법이 있지만 파드 ID를 매번 확인해야 하는것이 불편하다.</p>
<p>쿠버네티스 로그 열람을 돕는 도구로서 stern이 있는데 이는 레이블만 지정하면 로그확인이 가능하다.</p>
<p><code>stern -l app=echo</code> 라는 명령어를 통해 특정 파드가 삭제되거나 다시 생성되어도 로그를 볼 수 있다.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;구글-구글스택드라이버&quot;&gt;&lt;a href=&quot;#구글-구글스택드라이버&quot; class=&quot;headerlink&quot; title=&quot;구글 구글스택드라이버&quot;&gt;&lt;/a&gt;구글 구글스택드라이버&lt;/h3&gt;&lt;p&gt;구글 스택드라이버는 GCP나 AWS에서 로깅 및 모니터링에 사
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>1.네트워크 세상에 들어서며</title>
    <link href="http://KKimSangHeon.github.io/2019/06/19/network1/"/>
    <id>http://KKimSangHeon.github.io/2019/06/19/network1/</id>
    <published>2019-06-19T14:00:51.000Z</published>
    <updated>2019-06-25T11:46:52.012Z</updated>
    
    <content type="html"><![CDATA[<h3 id="인터넷"><a href="#인터넷" class="headerlink" title="인터넷"></a>인터넷</h3><p>여러개의 네트워크를 묶었다는 의미.<br><code>인터넷의 특징</code><br>1.TCP/IP라는 하나의 프로토콜만 사용<br>2.웹브라우저를 통해 인터넷을 탐험한다.<br>3.없는정보가 없다.</p>
<h3 id="인트라넷"><a href="#인트라넷" class="headerlink" title="인트라넷"></a>인트라넷</h3><p>내부의 네트워크를 의미. 사내업무도 웹 브라우저만으로 할 수 있다. TCP/IP 프로토콜을 사용함.</p>
<h3 id="엑스트라넷"><a href="#엑스트라넷" class="headerlink" title="엑스트라넷"></a>엑스트라넷</h3><p>인트라넷과 유사하지만 인트라넷을 기업의 종업원이외에도 협력회사나 고객에게 사용할 수 있도록 한것.</p>
<h3 id="LAN"><a href="#LAN" class="headerlink" title="LAN?"></a>LAN?</h3><p>Local Area Network의 약자로 한정된 공간에서 네트워크를 구성한다는것.(ex.한 사무실 내에 구축)</p>
<h3 id="WAN"><a href="#WAN" class="headerlink" title="WAN"></a>WAN</h3><p>멀리 떨어진 지역을 서로 연결하는 경우. 요즘은 LAN,WAN이 공존하는 형태이다.</p>
<h3 id="이더넷"><a href="#이더넷" class="headerlink" title="이더넷"></a>이더넷</h3><p>네트워킹의 한 방식으로 CSMA/CD라는 프로토콜을 사용해 통신한다. 우리나라의 <code>90%이상이 이더넷 방식을 사용함</code>.<br>네트워킹방식의 경우 다양한 방식이 존재한다.(토큰링,FDDI,ATM) 그러므로 이에 맞게 랜카드부터 네트워크 장비를 다르게 구입해야한다. 즉 자신의 네트워킹 방식을 모르고 랜카드 한장도 함부로 살수 없으므로 내용을 잘 알아두자.</p>
<p>CSMA/CD는 Carrier Sense Multiple Access/Collision Detection을 줄여서 대충 알아서 눈치로 통신하자라는 뜻이다.</p>
<p>이더넷 환경에서 통신을 하고싶은 PC나 서버는 네트워크상에 통신이 일어나고 있는지 확인(네트워크 자원을 쓰고있는 PC나 서버가 있는지 확인) 즉 캐리어가 있는지를 감지한다. 이를 Carrier Sense라고 한다. 캐리어가 감지되면 감지되지 않을때까지 기다렸다(눈치보다가) 자기 데이터를 네트워크에 실어 보낸다.</p>
<p>만약 동시에 여러 컴퓨터가 데이터를 보낼 경우 이를 Multiple Access라고 하며 Collision이 발생했다고 한다. 따라서 이더넷에서는 Collision Detection을 잘해야하며 발생했을 경우 랜덤한 시간동안 기다린 다음 다시 데이터를 전송하게 된다.</p>
<h3 id="토큰링"><a href="#토큰링" class="headerlink" title="토큰링?"></a>토큰링?</h3><p>한 네트워크에 토큰이 하나뿐이라(아닌경우도 있음) 한 네트워크에서 오직 한 PC, 즉 토큰을 가진 PC만이 네트워크에 데이터를 실어 보낼 수 있다.<br>보내고 난 후에는 옆 PC에게 토큰을 건네주게 되고 전송할 데이터가 없다면 다시 옆PC에게 전달한다. 이러한 방식으로 통신이 발생하여 충돌이 발생하지 않고 성능을 예측하기도 쉽다.<br>이더넷의 발전으로 역사의 뒤안길로 사라졌다..<br><code>단점</code>: 바로 보내야할 데이터가 있고 다른 PC들은 보낼 데이터가 하나도 없더라도 차례가 올때까지 기다려야 한다.</p>
<h3 id="이더넷-amp-토큰링"><a href="#이더넷-amp-토큰링" class="headerlink" title="이더넷 &amp; 토큰링"></a>이더넷 &amp; 토큰링</h3><p>데이터 네트워크의 두 가지 형태로서 이더넷을 많이쓰며 이더넷의 일반적인 속도는 100/1000Mbps이다. 토큰링은 100/1000Mbps이다.</p>
<h3 id="UTP-케이블"><a href="#UTP-케이블" class="headerlink" title="UTP 케이블"></a>UTP 케이블</h3><p>장비와 장비의 연결에는 어떤 종류의 케이블이든 반드시 케이블이 들어가게 된다. 이러한 케이블의 종류에는 광케이블, UTP 케이블, 동축케이블등 다양하다. 이중에 가장 많이 사용되는 것은 UTP케이블이다.</p>
<p><code>TP 케이블?</code> : Twisted-pair의 약어로써 즉 꼬인녀석이라는 의미이다. TP에는 UTP와 STP가 존재하며 <code>UTP</code>는 Unshielded(감싸지 않은) UP를 의미하는데 우리가 주로 사용하는 케이블이다. <code>STP</code>는 Shielded로 케이블 주위를 어떤 절연체로 감싸서 만든것을 말한다. STP가 좀더 비싸고 성능이 좋다.<br>기존 UTP가 많았기에 UTP가 주를 이루게 되었고 STP는 주로 토큰링 쪽에서 많이 쓰이고 있다.</p>
<h3 id="케이블"><a href="#케이블" class="headerlink" title="케이블"></a>케이블</h3><p><code>10 Base T</code>에서 10은 속도를 나타낸다. 즉 10Mbps의 속도를 지원하는 케이블을 의미한다.<br>Base란 말은 케이블이 Baseband용 케이블이라는 것을 의미한다. 케이블의 종류에는 Baseband와 Broadband가 있는데 Baseband는 디지털 방식이고 Broadband는 아날로그 방식이라고 볼 수 있다.<br>T가 있는 자리에는 케이블의 종류 또는 이 케이블이 전송할 수 있는 최대거리가 나온다. 여기서는 케이블의 종류가 나온것으로 T란 TP(Twisted Pair)케이블이라는 것을 나타낸다. 이것이 바로 UTP 케이블을 나타낸다.<br>가령 T 대신 숫자가 나왔을 땐 최대 통신거리를 의미하는데 10 Base 5 일경우 최대 500m까지 통신이 가능함을 의미한다.</p>
<p>즉 10 Base T는 10M의 속도로 최대 500미터까지 전송이 가능한 케이블을 의미한다.</p>
<h3 id="맥어드레스"><a href="#맥어드레스" class="headerlink" title="맥어드레스"></a>맥어드레스</h3><p>MAC는 Media Access Control의 약어로서 네트워크 상에서 어떻게 서로를 구분해서 인식할지에 대한것이다. 통신을 위해서는 서로를 구분할 일종의 주소가 필요한데 이를 MAC주소라 한다. 6옥테트(48bit) 의 주소를 갖게 되는데 이 주소는 랜카드 또는 네트워크 장비에 고정되어 있는 주소이고 유일하다. 6개의 16진수로 표현되는데 앞의 6개는 OUI(Organizational Unique Identifier) 즉 생산자를 나타내고 뒤의 6개는 시리얼 넘버로 볼 수 있다.</p>
<p>ARP:Address Resolution Protocol:IP주소를 다시 MAC로 바꾸는 절차를 의미.<br>iconfig/all을 입력했을 때 Physical address를 맥 어드레스라 한다.</p>
<p><code>같은 네트워크에서의 통신</code><br>PC Y는 자기가 속한 네트워크에 있는 모든 PC에게 메시지를 보내는데 이때 메시지의 의미는 “여기 네트워크안에 Z있으면 통신하고싶으니 맥어드레스좀 알려줘라”라는 것이다. PC Z는 Y에게 자신의 맥 어드레스를 알려주게 되고 통신을 시작하게 된다.</p>
<p><code>다른 네트워크간 통신</code><br>호스트 Z가 다른네트워크에 있을 때 즉 라우터를 통해 가야하는 경우라 생각해보자.<br>이경우 Y가 브로드 캐스트를 보내도 Z는 메세지를 받아볼 수 없다(라우터는 브로드 캐스팅을 통과시켜 주지 않는다.). 이 경우 라우터는 Y가 브로드캐스트를 보내도 Z가 대답하지 못할것을 알게된다. 그 후 라우터는 Y에게 자신 즉 라우터의 맥 어드레스를 보내면서 “나한테 전달해주면 내가 Z에게 전달해준다고 메세지를 보냅니다.”</p>
<p>따라서 PC Y는 PC Z에게 정보를 보낼 때 받는 맥 어드레스를 라우터의 맥 어드레스로 해서 보내게 된다. 그 다음 라우터가 Z가 있는 네트워크로 넘겨준다. 그럼 그곳에 살고있는 라우터는 Z의 맥 어드레스를 알아내고 해당 목적지로 데이터를 전송해준다.</p>
<p>이번에는 네트워크 통신방식에 대해 알아보자.</p>
<h3 id="유니캐스트"><a href="#유니캐스트" class="headerlink" title="유니캐스트"></a>유니캐스트</h3><p>네트워크상에서 가장 많이 사용되고 있는 트래픽이다. <u>출발지와 목적지의 주소 즉 맥어드레스를 프레임 안에 세팅하고 통신하는 방식을 의미한다. (받는 PC의 주소를 프레임 안에 하나 넣음)</u><br>특정 PC가 유니캐스트 프레임을 뿌리면 로컬 이더넷의 기본 성격이 붙어있는 모든 PC들을 일단 이프레임을 받아들여서 랜카드에서 자신의 맥 어드레스와 비교하게 된다. <u>자신이 받아야할것이라고 판단되면 받아서 처리하지만 자신이 받지말아야할 프레임일경우 프레임을 버리게 된다. 이 때는 PC의 CPU까지는 영향을 주지 않기 때문에 PC의 성능이 저하되는 일은 발생하지 않는다.</u> 즉 자신의 것이 아니면 랜카드가 해당 프레임을 버리고 자신의 것이면 CPU로 올려보내준다.</p>
<h3 id="브로드캐스트"><a href="#브로드캐스트" class="headerlink" title="브로드캐스트"></a>브로드캐스트</h3><p>로컬 랜 상에 붙어있는 모든 네트워크 장비들에게 보내는 통신. 로컬랜이란 라우터에 의해 구분지어진 공간, 즉 브로드 캐스트 도메인이라고 하는공간을 뜻한다. 브로드캐스트는 통신의 대상이 특정한 어떤 한 네트워크 장비가 아니고 내가 살고 있는 네트워크 안의 모든 네트워크 장비들에게 통신할 때 쓰기위한 방식이다. 브로드 캐스트의 주소는 FFFF.FFFF.FFFF이며 이 주소를 받은 랜카드는 해당 패킷을 CPU로 올려 보내게 되고 CPU는 이를 처리하게 된다. 이로인해 유니캐스트에 비해 CPU에 부담에 많아지는 방식이다. (해당 패킷을 받으면 CPU는 하던일을 중지하고 브로드캐스트 받은 것을 처리한다.)</p>
<p><code>브로드캐스트가 발생하는 경우</code> : ARP - 우리동네 사는 모든 사람들에게 이 IP주소 가진사람 누구야 라고 브로드캐스팅을 보내고 응답을 통해 알아낸다.</p>
<h3 id="멀티캐스트"><a href="#멀티캐스트" class="headerlink" title="멀티캐스트"></a>멀티캐스트</h3><p>200명중 150명에게만 데이터를 보내야 할경우 150명에게 유니캐스트를 보내는것은 150번을 반복해야 하므로 트래픽을 가중시키는 행위이다. 200명 모두에게 전송하는 브로드 캐스팅 또한 불합리한 방식이다.<br>이중 적합한 방법은 멀티캐스트이다. 멀티캐스트는 보내고자 하는 그룹 멤버들에게만 한번에 보낼 수 있으며 특정 그룹에 속해있는 사람들에게만 선택적으로, 한번에 보낼 수 있다. 만약 라우터나 스위치가 멀티캐스트를 지원하지 않는경우 이를 브로드캐스트처럼 취급해서 다 막아버리는 경우가 생길 수 있다.(라우터는 브로드 캐스트를 막는다)</p>
<h3 id="OSI7계층"><a href="#OSI7계층" class="headerlink" title="OSI7계층"></a>OSI7계층</h3><p>통신에 관한 국제적인 표준기구인 International Organization for Standardization(ISO)라는 곳에서 만든 OSI7레이어는 통신이 일어나는 단계를 7단게로 나눔.<br>장점</p>
<ul>
<li>데이터의 흐름이 한눈에 보인다.</li>
<li>문제 해결이 편리하다.(작은 문제들로 나누어져있음.)<br>메일이 안보내지면 ping를 통해 연결되었나 확인하고 되어있을경우 네트워크 영역까지는 잘 동작하는것이라 판단가능(피지컬.데이터링크,네트워크)</li>
<li>층별로 표준화를 하므로 여러 회사 장비를 써도 이상없이 돌아간다.(케이블 국산, 랜카드 인텔, 스위치나 허브는 시스코)</li>
</ul>
<p><code>피지컬 계층</code><br>전기,기계,기능적인 특성을 이용해서 통신 케이블로 데이터를 전송한다. 단지 데이터를 전달만 할뿐 에러, 통신의 효율등에는 관여하지 않음. 장비로는 통신케이블,리피터, 허브등이 있다.</p>
<p><code>데이터 링크 계층</code><br>정보의 오류와 흐름을 관리하여 안전한 정보의 전달 수행. 통신의 오류도 찾아주고 재전송도 하는 기능을 갖고있을 뿐 아니라 맥어드레스를 가지고 통신할 수 있게 해줌. 여기서 전송되는 단위를 프레임이라 함. 브릿지, 스위치 등이 있다.</p>
<p><code>네트워크 계층</code><br>데이터를 목적지까지 빠르고 안전하게 전달하는 라우팅을 한다. 라우터가 이 계층에 속하며 스위치 중에서도 라우팅 기능을 하는것들이 있기에 스위치를 보통 Layer3 스위치라 한다.</p>
<h3 id="프로토콜"><a href="#프로토콜" class="headerlink" title="프로토콜"></a>프로토콜</h3><p>규약 협약이란 뜻으로 인터넷을 사용하기 위해서는 모든 PC가 TCP/IP라는 프로토콜을 사용해야 한다. 즉 프로토콜이란 컴퓨터끼리 서로 통신하기 위해서 꼭 필요한 서로간의 통신규약 또는 통신방식에 대한 약속이다.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;인터넷&quot;&gt;&lt;a href=&quot;#인터넷&quot; class=&quot;headerlink&quot; title=&quot;인터넷&quot;&gt;&lt;/a&gt;인터넷&lt;/h3&gt;&lt;p&gt;여러개의 네트워크를 묶었다는 의미.&lt;br&gt;&lt;code&gt;인터넷의 특징&lt;/code&gt;&lt;br&gt;1.TCP/IP라는 하나의 프로토콜
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="Network" scheme="http://KKimSangHeon.github.io/categories/CS/Network/"/>
    
    
  </entry>
  
  <entry>
    <title>15. 로그 관리방법(Fluentd, Elasticsearch 활용)</title>
    <link href="http://KKimSangHeon.github.io/2019/06/16/kube18/"/>
    <id>http://KKimSangHeon.github.io/2019/06/16/kube18/</id>
    <published>2019-06-16T11:50:45.000Z</published>
    <updated>2019-06-23T07:37:39.594Z</updated>
    
    <content type="html"><![CDATA[<p>도커에서는 로그 라이브러리를 사용한다 해도 로그를 파일이 아닌 표준 출력으로 출력하고 이를 Fluentd 같은 로그 컬렉터로 수집하는 경우가 많다. 이를 활용할 경우 어플리케이션 쪽에서 로그 로테이션을 할 필요가 없으며 로그 전송을 돕는 로깅 드라이버 기능도 갖추고 있으므로 로그 수집이 편리하다.<br><code>로그 로테이션</code> : 로그로테이션이란 일정시간 주기로 원본 로그 파일을 다른 이름으로 복사하고, 로그 파일을 truncate 해서 새로 로그를 쌓게 하는 일련의 과정을 말한다. 일정 시간이 지난 로그파일은 알아서 삭제를 해주기 때문에 로그파일이 계속 커지는 것을 막을 수 있다. 참고:<a href="https://www.joinc.co.kr/w/man/12/logrotate" target="_blank" rel="external">https://www.joinc.co.kr/w/man/12/logrotate</a></p>
<h3 id="도커의-로깅-드라이버"><a href="#도커의-로깅-드라이버" class="headerlink" title="도커의 로깅 드라이버"></a>도커의 로깅 드라이버</h3><p>도커 컨테이너의로그는 JSON 포멧으로 출력되는데 이는 도커에 json-file이라는 기본 로깅 드라이버가 있기 때문이다. json-file 외에도 다양한 것들을 사용할 수 있다. 도커 로그는 fluentd를 사용하는것이 정석이며 퍼블릭 클라우드에서 도커를 사용하는 경우에는 awslogs나 gcplogs를 많이 사용한다.</p>
<p><code>Syslog</code> : syslog로 로그를 관리<br><code>Journald</code> : systemd로 로그 관리<br><code>Awslogs</code> : AWS CloudWatch Logs로 로그를 전송<br><code>Gcplogs</code> : Google Cloud Logging으로 로그 전송<br><code>Fluentd</code> : fluentd로 로그를 관리.</p>
<h3 id="컨테이너에서-로그-관리"><a href="#컨테이너에서-로그-관리" class="headerlink" title="컨테이너에서 로그 관리"></a>컨테이너에서 로그 관리</h3><p>컨테이너가 아닐 때 어플리케이션을 당연히 로그를 파일에 출력한다. 이 방법은 컨테이너에 적용하기에는 어려움이 있다. 장애로 인해 컨테이너가 날라갈경우 로그까지 날라가는 경우가 생길 수 있다. 도커처럼 로그를 표준 출력으로 남기면 로그가 호스트에 위치한 파일에 남는다. 물론 컨테이너에 공유한 볼륨에 파일로 로그를 남기는것도 가능하지만 <u>로그는 표준 출력으로 남기고 이 내용을 호스트에서 파일에 수집하는것이 더 간단하며 이것이 도커에서는 정석으로 여겨진다.</u></p>
<h3 id="쿠버네티스에서-로그-관리"><a href="#쿠버네티스에서-로그-관리" class="headerlink" title="쿠버네티스에서 로그 관리"></a>쿠버네티스에서 로그 관리</h3><p>컨테이너에서는 표준 출력으로만 로그를 내보내면 되고 이에 대한 처리는 컨테이너 외부에서 이뤄진다.</p>
<p>로컬 쿠버네티스 환경에 Elashticsearch와 kibana를 구축한 다음 로그를 전송할 fluentd와 DaemonSet을 구축해보자.</p>
<p><code>Kibana</code>: 로그 열람기능 활용.<br><code>Fluentd</code>: 로그 수집기. Elasticsearch로 전송하는 기능을 할 용도<br><code>Elasticsearch</code> : JVM에서 동작하는 풀텍스트 검색 엔진. fluentd로 수집한 로그를 검색하는 용도로도 사용할 수 있다. fluentd+Elasticsearch는 로그 관리에서 단골로 사용된다.</p>
<h3 id="로컬에서-Elashticsearch와-Kibana-구축하기"><a href="#로컬에서-Elashticsearch와-Kibana-구축하기" class="headerlink" title="로컬에서 Elashticsearch와 Kibana 구축하기"></a>로컬에서 Elashticsearch와 Kibana 구축하기</h3><p>kube-system 네임스페이스에 Elashticsearch를 구축해보자.<br>해당 네임스페이스는 쿠버네티스 클러스터의 기본 네임스페이스로 코어 컴포넌트가 배치된다. 로그는 네임스페이스를 뛰어넘어 Elashticsearch에 모여야 로그 검색에 편리하므로 Elashticsearch를 kube-system 네임스페이스에 배치한다.</p>
<p>1.엘라스틱서치 설치<br>PVC, 서비스, 디플로이먼트, 컨피그맵 등의 매니페스트를 포함한 다음 파일을 생성한다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div></pre></td><td class="code"><pre><div class="line">kind: PersistentVolumeClaim</div><div class="line">apiVersion: v1</div><div class="line">metadata:</div><div class="line">  name: elasticsearch-pvc</div><div class="line">  namespace: kube-system</div><div class="line">  labels:</div><div class="line">    kubernetes.io/cluster-service: &quot;true&quot;</div><div class="line">spec:</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  resources:</div><div class="line">    requests:</div><div class="line">      storage: 2G</div><div class="line"></div><div class="line">---</div><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: elasticsearch</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    app: elasticsearch</div><div class="line">  ports:</div><div class="line">  - protocol: TCP</div><div class="line">    port: 9200</div><div class="line">    targetPort: http</div><div class="line"></div><div class="line">---</div><div class="line">apiVersion: apps/v1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: elasticsearch</div><div class="line">  namespace: kube-system</div><div class="line">  labels:</div><div class="line">    app: elasticsearch</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: elasticsearch</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: elasticsearch</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: elasticsearch</div><div class="line">        image: elasticsearch:5.6-alpine</div><div class="line">        ports:</div><div class="line">        - containerPort: 9200</div><div class="line">          name: http</div><div class="line">        volumeMounts:</div><div class="line">        - mountPath: /data</div><div class="line">          name: elasticsearch-pvc</div><div class="line">        - mountPath: /usr/share/elasticsearch/config</div><div class="line">          name: elasticsearch-config</div><div class="line">      volumes:</div><div class="line">      - name: elasticsearch-pvc</div><div class="line">        persistentVolumeClaim:</div><div class="line">          claimName: elasticsearch-pvc</div><div class="line">      - name: elasticsearch-config</div><div class="line">        configMap:</div><div class="line">          name: elasticsearch-config</div><div class="line"></div><div class="line">---</div><div class="line">kind: ConfigMap</div><div class="line">apiVersion: v1</div><div class="line">metadata:</div><div class="line">  name: elasticsearch-config</div><div class="line">  namespace: kube-system</div><div class="line">data:</div><div class="line">  elasticsearch.yml: |-</div><div class="line">    http.host: 0.0.0.0</div><div class="line">    path.scripts: /tmp/scripts</div><div class="line"></div><div class="line">  log4j2.properties: |-</div><div class="line">    status = error</div><div class="line"></div><div class="line">    appender.console.type = Console</div><div class="line">    appender.console.name = console</div><div class="line">    appender.console.layout.type = PatternLayout</div><div class="line">    appender.console.layout.pattern = [%d&#123;ISO8601&#125;][%-5p][%-25c&#123;1.&#125;] %marker%m%n</div><div class="line"></div><div class="line">    rootLogger.level = info</div><div class="line">    rootLogger.appenderRef.console.ref = console</div><div class="line"></div><div class="line">  jvm.options: |-</div><div class="line">    -Xms128m</div><div class="line">    -Xmx256m</div><div class="line">    -XX:+UseConcMarkSweepGC</div><div class="line">    -XX:CMSInitiatingOccupancyFraction=75</div><div class="line">    -XX:+UseCMSInitiatingOccupancyOnly</div><div class="line">    -XX:+AlwaysPreTouch</div><div class="line">    -server</div><div class="line">    -Xss1m</div><div class="line">    -Djava.awt.headless=true</div><div class="line">    -Dfile.encoding=UTF-8</div><div class="line">    -Djna.nosys=true</div><div class="line">    -Djdk.io.permissionsUseCanonicalPath=true</div><div class="line">    -Dio.netty.noUnsafe=true</div><div class="line">    -Dio.netty.noKeySetOptimization=true</div><div class="line">    -Dio.netty.recycler.maxCapacityPerThread=0</div><div class="line">    -Dlog4j.shutdownHookEnabled=false</div><div class="line">    -Dlog4j2.disable.jmx=true</div><div class="line">    -Dlog4j.skipJansi=true</div><div class="line">    -XX:+HeapDumpOnOutOfMemoryError</div></pre></td></tr></table></figure></p>
<p>2.키바나 설치</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: kibana</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    app: kibana</div><div class="line">  ports:</div><div class="line">  - protocol: TCP</div><div class="line">    port: 5601</div><div class="line">    targetPort: http</div><div class="line">    nodePort: 30050</div><div class="line">  type: NodePort</div><div class="line"></div><div class="line">---</div><div class="line">apiVersion: apps/v1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: kibana</div><div class="line">  namespace: kube-system</div><div class="line">  labels:</div><div class="line">    app: kibana</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: kibana</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: kibana</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: kibana</div><div class="line">        image: kibana:5.6</div><div class="line">        ports:</div><div class="line">        - containerPort: 5601</div><div class="line">          name: http</div><div class="line">        env:</div><div class="line">        - name: ELASTICSEARCH_URL</div><div class="line">          value: &quot;http://elasticsearch:9200&quot;</div></pre></td></tr></table></figure>
<p>kubectl get pod -n kube-system 으로 파드들이 잘 돌고있나 확인해보자</p>
<p>localhost:30050으로 접속하여 키바나가 제대로 동작하고 있는지 보자.<br>나의경우 localhost:30050 으로 접속이 되질 않아<code>minikube service kibana --url -n kube-system</code> 명령어를 통해 주소를 알아냈다.</p>
<p>3.DaemonSet로 fluentd 구축하기<br>DeamonSet은 파드를 관리하는 리소스로 모든 노드에 하나씩 배치된다. 로그 컬렉터같이 호스트마다 특정할 역할을 하는 에이전트를 두고자 할 때 적합하다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">apiVersion: apps/v1</div><div class="line">kind: DaemonSet</div><div class="line">metadata:</div><div class="line">  name: fluentd</div><div class="line">  namespace: kube-system</div><div class="line">  labels:</div><div class="line">    app: fluentd-logging</div><div class="line">    version: v1</div><div class="line">    kubernetes.io/cluster-service: &quot;true&quot;</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: fluentd-logging</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: fluentd-logging</div><div class="line">        version: v1</div><div class="line">        kubernetes.io/cluster-service: &quot;true&quot;</div><div class="line">    spec:</div><div class="line">      tolerations:</div><div class="line">      - key: node-role.kubernetes.io/master</div><div class="line">        effect: NoSchedule</div><div class="line">      containers:</div><div class="line">      - name: fluentd</div><div class="line">        image: fluent/fluentd-kubernetes-daemonset:elasticsearch</div><div class="line">        env:</div><div class="line">          - name: FLUENT_ELASTICSEARCH_HOST</div><div class="line">            value: &quot;elasticsearch&quot;</div><div class="line">          - name: FLUENT_ELASTICSEARCH_PORT</div><div class="line">            value: &quot;9200&quot;</div><div class="line">          - name: FLUENT_ELASTICSEARCH_SCHEME</div><div class="line">            value: &quot;http&quot;</div><div class="line">          - name: FLUENT_UID</div><div class="line">            value: &quot;0&quot;</div><div class="line">        resources:</div><div class="line">          limits:</div><div class="line">            memory: 200Mi</div><div class="line">          requests:</div><div class="line">            cpu: 100m</div><div class="line">            memory: 200Mi</div><div class="line">        volumeMounts:</div><div class="line">        - name: varlog</div><div class="line">          mountPath: /var/log</div><div class="line">        - name: varlibdockercontainers</div><div class="line">          mountPath: /var/lib/docker/containers</div><div class="line">          readOnly: true</div><div class="line">      terminationGracePeriodSeconds: 30</div><div class="line">      volumes:</div><div class="line">      - name: varlog</div><div class="line">        hostPath:</div><div class="line">          path: /var/log</div><div class="line">      - name: varlibdockercontainers</div><div class="line">        hostPath:</div><div class="line">          path: /var/lib/docker/containers</div></pre></td></tr></table></figure>
<p>위를 apply 하자. 이는 fluent/fluentd-kubernetes-daemonset:elasticsearch 이미지를 사용한다.<br>로그를 전달받을 Elasticsearch의 주소를 환경변수에 설정하고 데이터나 로그가 저장될 위치인 /var/lib/containers에 볼륨을 마운트한 다음 fluentd컨테이너에서 로그를 받아간다.</p>
<p>책에서 나온 예제를 그대로 따라하면 fluentd가 Crash났다고 뜬다.<br>로그를 찾아 구글링 해보니 환경변수에 FLUENT_UID / 0 을 추가하라고 해서 해결함.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: echo</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    app: echo</div><div class="line">  ports:</div><div class="line">  - protocol: TCP</div><div class="line">    port: 80</div><div class="line">    targetPort: http</div><div class="line">    nodePort: 30080</div><div class="line">  type: NodePort</div><div class="line"></div><div class="line">---</div><div class="line">apiVersion: apps/v1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: echo</div><div class="line">  labels:</div><div class="line">    app: echo</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: echo</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: echo</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: nginx</div><div class="line">        image: gihyodocker/nginx:latest</div><div class="line">        env:</div><div class="line">        - name: BACKEND_HOST</div><div class="line">          value: localhost:8080</div><div class="line">        - name: LOG_STDOUT</div><div class="line">          value: &quot;true&quot;</div><div class="line">        ports:</div><div class="line">        - name: http</div><div class="line">          containerPort: 80</div><div class="line">      - name: echo</div><div class="line">        image: gihyodocker/echo:latest</div><div class="line">        ports:</div><div class="line">        - containerPort: 8080</div></pre></td></tr></table></figure>
<p>위의 파일을 만들고 apply하자.<br> minikube service echo –url<br> 나온 주소로 curl 을 보내보자</p>
<h3 id="로그-확인"><a href="#로그-확인" class="headerlink" title="로그 확인"></a>로그 확인</h3><p>kibana로 접속하여 인덱스 패턴을 <code>logstash-*</code>로 생성한다. 이는 fluent/fluentd-kubernetes-daemonset 이미지의 기본 설정에 따라 생성된것이다.</p>
<p>discover에서 curl보낸것에 대한 로그들을 볼 수 있다. echo 컨테이너에서 어플리케이션이 출력한 log 필드에 등록되며 그 외 레이블, 파드명, 컨테이너 명등을 볼 수 있다.</p>
<h3 id="도커-쿠버네티스-로그-관리-원칙"><a href="#도커-쿠버네티스-로그-관리-원칙" class="headerlink" title="도커 쿠버네티스 로그 관리 원칙"></a>도커 쿠버네티스 로그 관리 원칙</h3><p>어플리케이션 로그는 모두 표준 출력으로 출력한다.<br>nginx 등의 미들웨어는 로그가 표준 출력으로 출력되도록 이미지를 빌드한다.<br>표준 출력으로 출력으로 출력되는 로그는 모두 JSON포멧으로 출력해 각 속성을 검색할 수 있게 한다.<br>쿠버네티스 환경에서는 fluentd/fluentd-kubernetes-daemonset를 포함하는 파드를 DaemonSet를 사용해 각 호스트에 배포한다.<br>쿠버네티스 리소스에는 적절히 레이블을 부여해 로그를 검색할 수 있게한다.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;도커에서는 로그 라이브러리를 사용한다 해도 로그를 파일이 아닌 표준 출력으로 출력하고 이를 Fluentd 같은 로그 컬렉터로 수집하는 경우가 많다. 이를 활용할 경우 어플리케이션 쪽에서 로그 로테이션을 할 필요가 없으며 로그 전송을 돕는 로깅 드
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>14. 쿠버네티스 배포 전략(롤링업데이트, 블루그린)</title>
    <link href="http://KKimSangHeon.github.io/2019/06/15/kube17/"/>
    <id>http://KKimSangHeon.github.io/2019/06/15/kube17/</id>
    <published>2019-06-15T12:30:25.000Z</published>
    <updated>2019-06-16T11:40:35.289Z</updated>
    
    <content type="html"><![CDATA[<p>어플리케이션을 컨테이너로 배포하는 시대에 들어오면서 배포 전략에도 변화가 생겼다. 컨테이너의 장점을 살려 작업을 자동화하고 서비스 무중단을 어떻게 유지해야 할지를 고민한다.</p>
<h3 id="롤링-업데이트"><a href="#롤링-업데이트" class="headerlink" title="롤링 업데이트"></a>롤링 업데이트</h3><p>디플로이먼트의 파드를 교체하는 전략은 .specs.strategy.type로 정의 되며 RollingUpdate(기본값), Recreate(기존 파드가 모두 삭제된 다음 새로운 파드를 생성함) 중 선택한다.</p>
<p>다음 코드를 통해 서비스와 디플로이먼트를 생성하고 롤링업데이트가 어떻게 진행되는지 보자. 해당 서비스는 get 요청에 버전정보를 리턴하는 서비스이다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: echo-version</div><div class="line">  labels:</div><div class="line">    app: echo-version</div><div class="line">spec:</div><div class="line">  ports:</div><div class="line">  - port: 80</div><div class="line">    targetPort: 8080</div><div class="line">  selector:</div><div class="line">    app: echo-version</div><div class="line">---</div><div class="line">apiVersion: apps/v1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: echo-version</div><div class="line">  labels:</div><div class="line">    app: echo-version</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: echo-version</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: echo-version</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: echo-version</div><div class="line">        image: gihyodocker/echo-version:0.1.0</div><div class="line">        ports:</div><div class="line">        - containerPort: 8080</div></pre></td></tr></table></figure></p>
<p>get요청을 지속적으로 보내어 버전정보를 출력하기 위한 파드를 정의하자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata:</div><div class="line">  name: update-checker</div><div class="line">  labels:</div><div class="line">    app: update-checker</div><div class="line">spec:</div><div class="line">  containers:</div><div class="line">  - name: kubectl</div><div class="line">    image: gihyodocker/fundamental:0.1.0</div><div class="line">    command:</div><div class="line">    - sh</div><div class="line">    - -c</div><div class="line">    - |</div><div class="line">      while true</div><div class="line">      do</div><div class="line">        APP_VERSION=`curl -s http://echo-version/`</div><div class="line">        echo &quot;[`date`] $APP_VERSION &quot;</div><div class="line">        sleep 1</div><div class="line">      done</div></pre></td></tr></table></figure></p>
<p>다음을 입력하여 배포하자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kubectl apply -f 생성한 파일(디플로이먼트,서비스)</div><div class="line">$ kubectl apply -f 생성한 파일(파드)</div></pre></td></tr></table></figure></p>
<p>출력되는 버전정보를 확인해보자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$  kubectl logs -f update-checker</div></pre></td></tr></table></figure></p>
<p>위에 이미지의 버전을 변경해보자. 그리고<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">image: gihyodocker/echo-version:0.1.0 -&gt;image: gihyodocker/echo-version:0.2.0</div></pre></td></tr></table></figure></p>
<p>반영 후 롤링업데이트를 확인해보자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ kubectl apply -f 수정된파일.</div><div class="line"></div><div class="line">$ kubectl logs -f update-checker</div></pre></td></tr></table></figure></p>
<p><code>재배포 과정</code>: 새로운 파드가 생성되고 있을 때 기존 파드는 여전히 Running이다. 이후 새로운 파드가 생성되어 실행상태가 되고 기존파드는 Terminating이 된다. 그 후 기존파드는 완전히 폐기된다.</p>
<h3 id="실행중인-컨테이너에-대한-헬스체크-설정"><a href="#실행중인-컨테이너에-대한-헬스체크-설정" class="headerlink" title="실행중인 컨테이너에 대한 헬스체크 설정"></a>실행중인 컨테이너에 대한 헬스체크 설정</h3><p>쿠버네티스의 컨테이너 헬스체크에는 livenessProbe, readinessProbe 기능이 있다.</p>
<p><code>livenessProbe</code>: 어플리케이션 헬스 체크 기능으로 어플리케이션이 의존하는 컨테이너 안의 파일이 존재하는지를 확인하는 용도로 사용된다.<br><code>readinessProbe</code> : 컨테이너 외부에서 HTTP 요청같은 트래픽을 발생시켜 이를 처리할 수 있는 상태인지를 확인하는 기능이다.</p>
<p>헬스체크 기능을 포함한 컨테이너는 Running 상태가 되어도 READY 가 0/1 로 나오다가 모든 헬스 체크를 통과하고 1/1로 된다.</p>
<h3 id="블루그린-배포"><a href="#블루그린-배포" class="headerlink" title="블루그린 배포"></a>블루그린 배포</h3><p>롤링업데이트는 강력하지만 새버전, 구버전이 동시에 존재하는 경우도 생길 수 있다. 이는 때로는 부작용을 가져올 수 있다. 이럴 때 사용할 수 있는것이 블루그린 배포인데 <code>블루그린은 새버전, 구버전의 2세트의 서버를 마련하고 한꺼번에 교체하는 배포 방법이다.</code> 이는 로드 밸런서 혹은 서비스 디스커버리 수준에서 참조 대상을 교체하는 방식으로 이뤄지는 배포형태이다. 이는 서버군이 아닌 켄테이너군을 통해 구현되고 배포할 서버군을 2게통으로 유지해야 하므로 롤링 업데이트보다 필요 리소스 양이 늘어난다. 그럼에도 다음 두가지의 장점을 누릴 수 있다.<br>1.신버전과 구버전이 혼재하는 시간없이 순간적인 교체가능<br>2.한쪽 서버군을 릴리즈 전 스탠바이 상태로 사용할 수 있음</p>
<h3 id="블루그린-배포-실습"><a href="#블루그린-배포-실습" class="headerlink" title="블루그린 배포 실습"></a>블루그린 배포 실습</h3><p>2세트를 미리 만둘어주자.</p>
<figure class="highlight xml"><figcaption><span>echo-version-blue.yaml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">apiVersion: apps/v1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: echo-version-blue</div><div class="line">  labels:</div><div class="line">    app: echo-version</div><div class="line">    color: blue</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: echo-version</div><div class="line">      color: blue</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: echo-version</div><div class="line">        color: blue</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: echo-version</div><div class="line">        image: gihyodocker/echo-version:0.1.0</div><div class="line">        ports:</div><div class="line">        - containerPort: 8080</div><div class="line"></div></pre></td></tr></table></figure>
<figure class="highlight xml"><figcaption><span>echo-version-green.yaml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">apiVersion: apps/v1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: echo-version-green</div><div class="line">  labels:</div><div class="line">    app: echo-version</div><div class="line">    color: green</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: echo-version</div><div class="line">      color: green</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: echo-version</div><div class="line">        color: green</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: echo-version</div><div class="line">        image: gihyodocker/echo-version:0.2.0</div><div class="line">        ports:</div><div class="line">        - containerPort: 8080</div><div class="line"></div></pre></td></tr></table></figure>
<p>echo-version-blue / echo-version-green으로 정의한 두개의 디플로이먼트파일을 준비하자. metadata.label을 주목해서 보자.</p>
<p>그 후 다음명령어를 통해 디플로이먼트를 배포하자<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kubectl apply -f echo-version-blue.yaml</div><div class="line">$ kubectl apply -f echo-version-green.yaml</div></pre></td></tr></table></figure></p>
<p>셀렉터 레이블을 변경해 디플로이먼트를 변경할 수 있다.<br><figure class="highlight xml"><figcaption><span>echo-version-service.yaml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: echo-version</div><div class="line">  labels:</div><div class="line">    app: echo-version</div><div class="line">spec:</div><div class="line">  ports:</div><div class="line">  - port: 80</div><div class="line">    targetPort: 8080</div><div class="line">  selector:</div><div class="line">    app: echo-version</div><div class="line">    color: blue</div><div class="line"></div><div class="line"></div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">kubectl apply -f echo-service</div><div class="line"></div><div class="line">kubectl logs -f update-checker</div></pre></td></tr></table></figure>
<p>위의 명령어를 통해 서비스를 배포하고 업데이트 체크를 해보자.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl patch service echo-version -p &apos;&#123;&quot;spec&quot;:&#123;&quot;selector&quot;:&#123;&quot;color&quot;:&quot;green&quot;&#125;&#125;&#125;&apos;</div></pre></td></tr></table></figure>
<p>위의 명령어를 입력하여 서비스 대상 디플로이먼트를 변경하자.</p>
<p>이후에 출력되는 버전을 보면 버전이 바뀐것을 확인할 수 있다.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;어플리케이션을 컨테이너로 배포하는 시대에 들어오면서 배포 전략에도 변화가 생겼다. 컨테이너의 장점을 살려 작업을 자동화하고 서비스 무중단을 어떻게 유지해야 할지를 고민한다.&lt;/p&gt;
&lt;h3 id=&quot;롤링-업데이트&quot;&gt;&lt;a href=&quot;#롤링-업데이트&quot; 
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>실습3. 프로젝트 생성 및 테스트</title>
    <link href="http://KKimSangHeon.github.io/2019/06/14/kube16/"/>
    <id>http://KKimSangHeon.github.io/2019/06/14/kube16/</id>
    <published>2019-06-14T10:34:59.000Z</published>
    <updated>2019-06-15T12:27:53.138Z</updated>
    
    <content type="html"><![CDATA[<p>깃랩에 프로젝트를 커밋하기 위해 다음 과정을 거친다.<br><img src="/2019/06/14/kube16/9.PNG" alt="9.PNG" title=""><br>다음과 같이 프로젝트를 깃랩에 생성 및 코드 반영<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">git config --global user.name &quot;sangheonKim&quot;</div><div class="line">git config --global user.email &quot;tkdgjs1501@nate.com&quot;</div><div class="line"></div><div class="line">git init</div><div class="line">git remote add origin http://35.200.116.34/sangheonKim/sangheonProject.git</div><div class="line">git commit -m &quot;init&quot;</div><div class="line">git push origin master</div></pre></td></tr></table></figure></p>
<p>커밋하는데 아래와 같은 에러가 뜰 경우 레파지토리 이름에 .git를 넣었는지 확인해보자.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">error: RPC failed; result=22, HTTP code = 404</div><div class="line">fatal: The remote end hung up unexpectedly</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div></pre></td><td class="code"><pre><div class="line">def label = &quot;master&quot;</div><div class="line"></div><div class="line"> def notifyStarted() &#123;</div><div class="line">    slackSend (color: &apos;#FFFF00&apos;, message: &quot;STARTED: Job &apos;$&#123;env.JOB_NAME&#125; [$&#123;env.BUILD_NUMBER&#125;]&apos; ($&#123;env.BUILD_URL&#125;)&quot;)</div><div class="line">&#125;</div><div class="line"></div><div class="line"> def notifySuccessful() &#123;</div><div class="line">    slackSend (color: &apos;#00FF00&apos;, message: &quot;SUCCESSFUL: Job &apos;$&#123;env.JOB_NAME&#125; [$&#123;env.BUILD_NUMBER&#125;]&apos; ($&#123;env.BUILD_URL&#125;)&quot;)</div><div class="line">&#125;</div><div class="line"></div><div class="line"> def notifyFailed() &#123;</div><div class="line">  slackSend (color: &apos;#FF0000&apos;, message: &quot;FAILED: Job &apos;$&#123;env.JOB_NAME&#125; [$&#123;env.BUILD_NUMBER&#125;]&apos; ($&#123;env.BUILD_URL&#125;)&quot;)</div><div class="line">&#125;</div><div class="line"></div><div class="line"> podTemplate(label: label,</div><div class="line">	containers: [</div><div class="line">	  containerTemplate(name: &apos;maven&apos;, image: &apos;maven:3.3.9-jdk-8-alpine&apos;, ttyEnabled: true, command: &apos;cat&apos;),</div><div class="line">	  containerTemplate(name: &apos;docker&apos;, image: &apos;docker:stable&apos;, ttyEnabled: true, command: &apos;cat&apos;),</div><div class="line">	  containerTemplate(name: &apos;kubectl&apos;, image: &apos;lachlanevenson/k8s-kubectl:v1.8.0&apos;, command: &apos;cat&apos;, ttyEnabled: true)</div><div class="line">	],</div><div class="line">	volumes: [hostPathVolume(hostPath: &apos;/var/run/docker.sock&apos;, mountPath: &apos;/var/run/docker.sock&apos;),</div><div class="line">		hostPathVolume(hostPath: &apos;/root/.m2&apos;, mountPath: &apos;/root/.m2&apos;)</div><div class="line">	]</div><div class="line"></div><div class="line"> 	) &#123;</div><div class="line"></div><div class="line"> 	node(label) &#123;</div><div class="line">	try &#123;</div><div class="line"></div><div class="line"></div><div class="line"> 	  stage(&apos;Get Source&apos;) &#123;</div><div class="line">		git &quot;http://35.200.116.34/sangheonKim/sangheonProject&quot;</div><div class="line">	  &#125;</div><div class="line"></div><div class="line"> 	  def props = readProperties  file:&apos;deployment/pipeline.properties&apos;</div><div class="line">	  /*def tag = VersionNumber (versionNumberString: &apos;$&#123;BUILD_DATE_FORMATTED, &quot;yyyyMMdd&quot;&#125;-develop-$&#123;BUILDS_TODAY&#125;&apos;)*/</div><div class="line">	  def tag = props[&apos;version&apos;]</div><div class="line">	  def gitSrc = props[&apos;gitSrc&apos;]</div><div class="line">	  def dockerRegistry = props[&apos;dockerRegistry&apos;]</div><div class="line">	  def image = props[&apos;image&apos;]</div><div class="line">	  def deployment = props[&apos;deployment&apos;]</div><div class="line">	  def service = props[&apos;service&apos;]</div><div class="line">	  def selector = props[&apos;selector&apos;]</div><div class="line">	  def namespace = props[&apos;namespace&apos;]</div><div class="line"></div><div class="line"></div><div class="line"> 	  stage(&apos;Build Maven project&apos;) &#123;</div><div class="line">		container(&apos;maven&apos;) &#123;</div><div class="line">			sh &quot;mvn -B clean package&quot;</div><div class="line">		&#125;</div><div class="line">	  &#125;</div><div class="line">/*</div><div class="line"> 	  stage(&apos;Inspection Code&apos;) &#123;</div><div class="line">		  container(&apos;maven&apos;) &#123;</div><div class="line">			  sh &quot;mvn sonar:sonar \</div><div class="line">        -Dsonar.projectKey=sangheon \</div><div class="line">        -Dsonar.host.url=http://34.85.31.212:9000 \</div><div class="line">        -Dsonar.login=62cf8d43f8fda31db0c5682c71b584c8650f4509&quot;</div><div class="line">		  &#125;</div><div class="line">	  &#125;</div><div class="line">*/</div><div class="line"> 	  stage(&apos;Build Docker image&apos;) &#123;</div><div class="line">		container(&apos;docker&apos;) &#123;</div><div class="line">		  docker.withRegistry(&quot;$&#123;dockerRegistry&#125;&quot;, &apos;registry-credentials&apos;) &#123;</div><div class="line">			sh &quot;docker build -t $&#123;image&#125;:$&#123;tag&#125; .&quot;</div><div class="line">			sh &quot;docker push $&#123;image&#125;:$&#123;tag&#125;&quot;</div><div class="line">			sh &quot;docker tag $&#123;image&#125;:$&#123;tag&#125; $&#123;image&#125;:latest&quot;</div><div class="line">			sh &quot;docker push $&#123;image&#125;:latest&quot;</div><div class="line">		  &#125;</div><div class="line">		&#125;</div><div class="line">	  &#125;</div><div class="line"></div><div class="line"> 	  stage( &apos;Clean Up Existing Deployments&apos; ) &#123;</div><div class="line">		container(&apos;kubectl&apos;) &#123;</div><div class="line">		  withCredentials([[$class: &apos;UsernamePasswordMultiBinding&apos;,</div><div class="line">							  credentialsId: &apos;registry-credentials&apos;,</div><div class="line">							  usernameVariable: &apos;DOCKER_HUB_USER&apos;,</div><div class="line">							  passwordVariable: &apos;DOCKER_HUB_PASSWORD&apos;]]) &#123;</div><div class="line"></div><div class="line"> 			  sh &quot;kubectl delete deployments -n $&#123;namespace&#125; --selector=app=$&#123;selector&#125;&quot;</div><div class="line">		  &#125;</div><div class="line">		&#125;</div><div class="line">	  &#125;</div><div class="line"></div><div class="line"> 	  stage( &apos;Deploy to Cluster&apos; ) &#123;</div><div class="line">		container(&apos;kubectl&apos;) &#123;</div><div class="line">		  withCredentials([[$class: &apos;UsernamePasswordMultiBinding&apos;,</div><div class="line">							  credentialsId: &apos;registry-credentials&apos;,</div><div class="line">							  usernameVariable: &apos;DOCKER_HUB_USER&apos;,</div><div class="line">							  passwordVariable: &apos;DOCKER_HUB_PASSWORD&apos;]]) &#123;</div><div class="line"></div><div class="line"> 			  sh &quot;kubectl apply -n $&#123;namespace&#125; -f $&#123;deployment&#125;&quot;</div><div class="line">			  sh &quot;sleep 5&quot;</div><div class="line">			  sh &quot;kubectl apply -n $&#123;namespace&#125; -f $&#123;service&#125;&quot;</div><div class="line">		  &#125;</div><div class="line">		&#125;</div><div class="line">	  &#125;</div><div class="line"></div><div class="line"> 	  notifySuccessful()</div><div class="line">	  &#125; catch(e) &#123;</div><div class="line">	print(e)</div><div class="line">        currentBuild.result = &quot;FAILED&quot;</div><div class="line">        notifyFailed()</div><div class="line">    &#125;</div><div class="line"></div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">version=0.1.0</div><div class="line">cluster=devcluster.icp</div><div class="line">namespace=default</div><div class="line">dockerRegistry=http://docker-registry.default.svc.cluster.local:5000</div><div class="line">image=http://docker-registry.default.svc.cluster.local:5000/default/masterapi</div><div class="line">deployment=deployment/masterapi-deploy.yaml</div><div class="line">service=deployment/masterapi-svc.yaml</div><div class="line">selector=masterapi</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: masterapi-sangheonkim</div><div class="line">spec:</div><div class="line">  replicas: 3</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: masterapi-sangheonkim</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">        - name: masterapi-sangheonkim</div><div class="line">          image: http://docker-registry.default.svc.cluster.local:5000/default/masterapi</div><div class="line">          imagePullPolicy: Always</div><div class="line">          ports:</div><div class="line">            - containerPort: 8280</div></pre></td></tr></table></figure>
<p>다음 그림처럼 파이프라인을 생성하자</p>
<p>kubectl create clusterrolebinding default-binding –clusterrole=cluster-admin –user=admin –user=kubelet –group=system:serviceaccounts:default</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Running in Durability level: MAX_SURVIVABILITY</div><div class="line">[Pipeline] Start of Pipeline</div><div class="line">[Pipeline] podTemplate</div><div class="line">[Pipeline] &#123;</div><div class="line">[Pipeline] node</div><div class="line">Still waiting to schedule task</div><div class="line">Waiting for next available executor</div></pre></td></tr></table></figure>
<p>위와 같은 오류가 뜬다면</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;깃랩에 프로젝트를 커밋하기 위해 다음 과정을 거친다.&lt;br&gt;&lt;img src=&quot;/2019/06/14/kube16/9.PNG&quot; alt=&quot;9.PNG&quot; title=&quot;&quot;&gt;&lt;br&gt;다음과 같이 프로젝트를 깃랩에 생성 및 코드 반영&lt;br&gt;&lt;figure clas
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>실습2. 젠킨스 설정(깃랩, 도커레지스트리,슬랙, 소나큐브 연동)</title>
    <link href="http://KKimSangHeon.github.io/2019/06/11/kube15/"/>
    <id>http://KKimSangHeon.github.io/2019/06/11/kube15/</id>
    <published>2019-06-11T10:57:54.000Z</published>
    <updated>2019-06-14T11:15:22.586Z</updated>
    
    <content type="html"><![CDATA[<h3 id="젠킨스-플러그인-설치"><a href="#젠킨스-플러그인-설치" class="headerlink" title="젠킨스 플러그인 설치"></a>젠킨스 플러그인 설치</h3><p>Jenkins관리 -&gt; 플러그인 관리에 들어가서 다음 항목을 설치하자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Build Pipeline(Build Pipeline Plugin?)</div><div class="line">Aqua MicroScanner</div><div class="line">Gitlab Authentication plugin</div><div class="line">GitLab Plugin</div><div class="line">Kubernetes</div><div class="line">Pipeline</div><div class="line">Pipeline Utility Steps</div><div class="line">Simple Theme Plugin</div><div class="line">Slack Notification</div><div class="line">SonarQube Scanner</div><div class="line">Version Number Plug-In</div></pre></td></tr></table></figure></p>
<h3 id="젠킨스-설정"><a href="#젠킨스-설정" class="headerlink" title="젠킨스 설정"></a>젠킨스 설정</h3><p><code>1. 깃랩과 연결</code><br><img src="/2019/06/11/kube15/gitlab_maketoken.PNG" alt="깃랩에서 설정" title="깃랩에서 설정"><br>우측 상단 이미지를 클릭하고 User Setting로 들어가서 Access Token 탭을 선택한다.<br>3개 모두 체크하고 토큰을 생성하자.</p>
<img src="/2019/06/11/kube15/jenkins_1.PNG" alt="젠킨스에서 깃랩연결" title="젠킨스에서 깃랩연결">
<p>Jenkins 관리-&gt; 시스템 설정으로 들어간 후 위와같이 connection name, gitlab host url(이전에 생성했던) 설정</p>
<p>위의 화면의 Credentials 옆 Add-&gt;Jenkins를 선택</p>
<img src="/2019/06/11/kube15/jenkins_2.PNG" alt="젠킨스에서 깃랩연결" title="젠킨스에서 깃랩연결">
<p>Kind를 위와 같이 선택하고 API token, Id, Description 입력하고 추가하고 Test Connection을 눌러 잘 동작하나 확인해보자.</p>
<img src="/2019/06/11/kube15/jenkins_3.PNG" alt="젠킨스에서 깃랩연결" title="젠킨스에서 깃랩연결">
<p>Jenkins location을 설정하고 email을 위와같이 설정</p>
<p><code>2. 도커레지스트리와 연결</code><br><img src="/2019/06/11/kube15/dockerConnection1.PNG" alt="젠킨스에서 깃랩연결" title="젠킨스에서 깃랩연결"><br>위와같이 설정</p>
<img src="/2019/06/11/kube15/dockerConnection2.PNG" alt="젠킨스에서 깃랩연결" title="젠킨스에서 깃랩연결">
<p>Add 버튼 클릭</p>
<p><code>3. 슬랙과 연동</code><br>슬랙에서 Apps선택하고 jenkins검색하여 view를 클릭하면 새 창이 나온다.</p>
<img src="/2019/06/11/kube15/1.PNG" alt="1.PNG" title="">
<p>Add Configuration 클릭</p>
<img src="/2019/06/11/kube15/2.PNG" alt="2.PNG" title="">
<p>노티를 보낼 슬랙을 선택 후 Add Jenkins CI integration 클릭</p>
<img src="/2019/06/11/kube15/3.PNG" alt="3.PNG" title="">
<p>다음과 같이 친절하게 스탭별로 알려주므로 따라하면 된다.</p>
<img src="/2019/06/11/kube15/4.PNG" alt="4.PNG" title="">
<p>스텝에서 설명하는 그림과 약간 다를 경우 Integration Token Credential ID 옆의 Add를 누르고 Secret text로 선택하여 추가하면 된다.</p>
<p>그 후 Test Connection을 클릭하여 테스트해보자</p>
<p><code>4. 소나큐브 연동</code><br>생성한 소나큐브에 접속 및 admin/admin으로 로그인</p>
<img src="/2019/06/11/kube15/5.PNG" alt="5.PNG" title="">
<p>Create new project 선택</p>
<img src="/2019/06/11/kube15/6.PNG" alt="6.PNG" title="">
<p>다음과 같이 프로젝트키, 디스플레이 네임 입력.</p>
<img src="/2019/06/11/kube15/7.PNG" alt="7.PNG" title="">
<p>생성된 토큰을 기록해둔다.</p>
<img src="/2019/06/11/kube15/8.PNG" alt="8.PNG" title="">
<p>Continue를 클릭하여 다음과같이 진행한다.(프로젝트의 종류에 따라 다를 수 있음)</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;젠킨스-플러그인-설치&quot;&gt;&lt;a href=&quot;#젠킨스-플러그인-설치&quot; class=&quot;headerlink&quot; title=&quot;젠킨스 플러그인 설치&quot;&gt;&lt;/a&gt;젠킨스 플러그인 설치&lt;/h3&gt;&lt;p&gt;Jenkins관리 -&amp;gt; 플러그인 관리에 들어가서 다음 항
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>실습1. helm을 이용한 툴 설치</title>
    <link href="http://KKimSangHeon.github.io/2019/06/11/kube14/"/>
    <id>http://KKimSangHeon.github.io/2019/06/11/kube14/</id>
    <published>2019-06-11T10:57:50.000Z</published>
    <updated>2019-06-15T14:58:59.586Z</updated>
    
    <content type="html"><![CDATA[<h3 id="깃랩설치"><a href="#깃랩설치" class="headerlink" title="깃랩설치"></a>깃랩설치</h3><p>1.깃랩설치<br>helm install –name sangheon-gitlab –set externalUrl=<a href="http://sangheon-gitlab.com" target="_blank" rel="external">http://sangheon-gitlab.com</a> stable/gitlab-ce</p>
<p>2.접속<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ kubectl get svc -w sangheon-gitlab-gitlab-ce  # 설치가 잘 되었나 확인</div><div class="line"></div><div class="line">$ export SERVICE_IP=$(kubectl get svc --namespace default sangheon-gitlab-gitlab-ce -o jsonpath=&apos;&#123;.status.loadBalancer.ingress[0].ip&#125;&apos;)</div><div class="line">$ echo http://$SERVICE_IP/  # 접속해보자</div></pre></td></tr></table></figure></p>
<p>처음 접속하면 root 계정의 password를 설정하게 된다. 설정하고 root로 로그인해보자.</p>
<h3 id="젠킨스-설치"><a href="#젠킨스-설치" class="headerlink" title="젠킨스 설치"></a>젠킨스 설치</h3><p>1.젠킨스 설치<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ helm install --name sangheonkim-jenkins stable/jenkins</div></pre></td></tr></table></figure></p>
<p>2.접속<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ printf $(kubectl get secret --namespace default sangheonkim-jenkins -o jsonpath=&quot;&#123;.data.jenkins-admin-password&#125;&quot; | base64 --decode);echo</div><div class="line"></div><div class="line">출력값을 잘 기억하자. admin의 password이다.</div><div class="line"></div><div class="line">$ kubectl get svc --namespace default -w sangheonkim-jenkins</div><div class="line">$ export SERVICE_IP=$(kubectl get svc --namespace default sangheonkim-jenkins --template &quot;&#123;&#123; range (index .status.loadBalancer.ingress 0) &#125;&#125;&#123;&#123; . &#125;&#125;&#123;&#123; end &#125;&#125;&quot;)</div><div class="line">$ echo http://$SERVICE_IP:8080/login</div></pre></td></tr></table></figure></p>
<p>출력된 주소로 접속하여 admin/출력된 password를 입력하자.</p>
<h3 id="소나큐브-설치"><a href="#소나큐브-설치" class="headerlink" title="소나큐브 설치"></a>소나큐브 설치</h3><p>1.설치</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ helm install --name sangheonkim-sonarqube stable/sonarqube</div></pre></td></tr></table></figure>
<p>2.접속<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ kubectl get svc -w sangheonkim-sonarqube-sonarqube</div><div class="line"></div><div class="line">$ export SERVICE_IP=$(kubectl get svc --namespace default sangheonkim-sonarqube-sonarqube -o jsonpath=&apos;&#123;.status.loadBalancer.ingress[0].ip&#125;&apos;)</div><div class="line"></div><div class="line">$ echo http://$SERVICE_IP:9000</div></pre></td></tr></table></figure></p>
<h3 id="dockerRegistry-구축"><a href="#dockerRegistry-구축" class="headerlink" title="dockerRegistry 구축"></a>dockerRegistry 구축</h3><p>1.설치<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ helm install --name docker-registry stable/docker-registry</div></pre></td></tr></table></figure></p>
<p>2.접속<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ export POD_NAME=$(kubectl get pods --namespace default -l &quot;app=docker-registry,release=docker-registry&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)</div><div class="line"></div><div class="line">$ echo &quot;Visit http://127.0.0.1:8080</div><div class="line"></div><div class="line">$ kubectl port-forward $POD_NAME 8080:5000</div></pre></td></tr></table></figure></p>
<p> <a href="http://127.0.0.1:8080" target="_blank" rel="external">http://127.0.0.1:8080</a> 로 앱에서 접속가능한듯?<br> 포트포워딩을 통해 5000으로도 가능?<br><a href="http://docker-registry.default.svc.cluster.local:5000" target="_blank" rel="external">http://docker-registry.default.svc.cluster.local:5000</a></p>
<p>/</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;깃랩설치&quot;&gt;&lt;a href=&quot;#깃랩설치&quot; class=&quot;headerlink&quot; title=&quot;깃랩설치&quot;&gt;&lt;/a&gt;깃랩설치&lt;/h3&gt;&lt;p&gt;1.깃랩설치&lt;br&gt;helm install –name sangheon-gitlab –set externalUrl=
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>MSA이론4. DevOps</title>
    <link href="http://KKimSangHeon.github.io/2019/06/08/msa4/"/>
    <id>http://KKimSangHeon.github.io/2019/06/08/msa4/</id>
    <published>2019-06-08T05:19:44.000Z</published>
    <updated>2019-06-11T11:05:58.469Z</updated>
    
    <content type="html"><![CDATA[<h3 id="DevOps"><a href="#DevOps" class="headerlink" title="DevOps"></a>DevOps</h3><p>Using DevOps to Solve your Delivery Challenges</p>
<ul>
<li>배포는 너무 오래걸린다.</li>
<li>테스트에 대해 혼란스럽다.</li>
<li>특정 개인에 대한 높은 의존성.</li>
<li>데브옵스 모델은 lock을 제거한다.</li>
<li>데브옵스 모델은 프로비저닝한다.</li>
</ul>
<p><code>Delivery는 Continuous integration + continuous deployment를 의미한다.</code></p>
<h4 id="데프옵스의-특징"><a href="#데프옵스의-특징" class="headerlink" title="데프옵스의 특징"></a>데프옵스의 특징</h4><ul>
<li>높은신뢰성, 높은 퍼포먼스</li>
<li>데브옵스는 프로덕트, 툴이아니다. 데브옵스는 문화이다.</li>
<li>고수준의 자동화 프로세스</li>
<li>정기적이고 반복적인 플로우를 따른다. 싸이클은 짧고, 작고 잦은 변경을 선호한다.</li>
</ul>
<h4 id="DevOps-1"><a href="#DevOps-1" class="headerlink" title="DevOps"></a>DevOps</h4><ul>
<li>데브옵스의 철학은 개발과 운영의 통합 문화를 만들고, 협업 변화를 촉진하기 위해 기술의 linked toolchain 을 요구하는 것이다.<br><code>toolchain?</code> : Target 시스템의 Software 개발을 진행하기 위해 필요한 host system의 Cross Compile(교차 컴파일) 환경이다.</li>
<li>DevOps의 toolchain은 수십개의 non-collaborative 툴을 포함할 수 있고, 복잡하고 어려운 작업을 자동화를 통해 task로 만들 수 있다.</li>
<li>DepOps의 정의에 따르면 one-size-fits-all이 해결책이 아니다.</li>
<li>DevOps는 특정 문제 및 과제에 대한 정보를 제공한다.</li>
</ul>
<h4 id="DevOps를-위한-microservice-requirements"><a href="#DevOps를-위한-microservice-requirements" class="headerlink" title="DevOps를 위한 microservice requirements"></a>DevOps를 위한 microservice requirements</h4><ul>
<li>microservice에 기반한 새로운 어플리케이션을 빌드하라</li>
<li>기존 microservice 어플리케이션을 클라우드로 마이그레이션하라</li>
<li>하이브리드 microservice 어플리케이션을 배포하라</li>
<li>존재하는 어플리케이션들을 microservice architecture로 발전하라.</li>
</ul>
<h4 id="Microservice-배포를-위해-따라야-할-Roles"><a href="#Microservice-배포를-위해-따라야-할-Roles" class="headerlink" title="Microservice 배포를 위해 따라야 할 Roles"></a>Microservice 배포를 위해 따라야 할 Roles</h4><ul>
<li>개발자는 테스트를 반복하며 각 테스트는 pass해야한다. 또한 계속되는 피드백에 대비하자.</li>
<li>테스터는 자동화 된 파이프라인에 컴포넌트, 통합, 회귀를 포함하고 크로스 플랫폼간 API를 통해 테스트가 가능토록 하자.</li>
<li>성능테스터는 로딩 테스팅을 모니터링하고 blue/green, rolling 배포를 하자.</li>
</ul>
<p><code>rolling 배포:</code>일반적인 배포를 의미하는데, 단순하게 한 대씩 재시작한다. 코드 변경에 따른 side effect가 발생할 수 있다<br><code>blue-green 배포:</code> 예전 배포물을 블루(blue), 신규 배포물을 그린(green)이라고 해서 붙여진 이름이다. 새로운 배포물을 배포하고 모든 연결을 새로운 배포물만 보게 하며 코드 변경에 따른 side effect가 없다. (배포시 중단시점이 없음!)</p>
<ul>
<li>Release 하는사람은 성능 측정을 해야한다.</li>
<li>운영자는 가용성을 모니터링하고 blue/green 배포를 해야한다.</li>
<li>엔드유저는 지속적인 피드백을 주고 변경에 대해 대비한다.</li>
</ul>
<h4 id="DevOps의-평가기준"><a href="#DevOps의-평가기준" class="headerlink" title="DevOps의 평가기준"></a>DevOps의 평가기준</h4><p>배포빈도(얼마나 자주 배포하나)</p>
<ul>
<li>최상: 요구될때마다</li>
<li>보통: 일주일 혹은 한달에 한번</li>
<li>최하: 한달에서 여섯달 사이에 한번</li>
</ul>
<p>Lead time for changes(ie.코드가 커밋되고 정상적으로 동작하는데 걸리는 시간)</p>
<ul>
<li>최상: 한시간 이내</li>
<li>보통: 일주일 ~ 한달사이</li>
<li>최하: 한달~여섯달 사이</li>
</ul>
<p>Mean time to recover(복구하는데 걸리는 시간)</p>
<ul>
<li>최상: 한시간 이내</li>
<li>보통: 하루 이상</li>
<li>최하: 하루 이상</li>
</ul>
<p>Change failure rate</p>
<ul>
<li>최상: 0~15%</li>
<li>보통: 31~45%</li>
<li>최하: 16~30%</li>
</ul>
<h4 id="Database-관련-생각해봐야-할것들"><a href="#Database-관련-생각해봐야-할것들" class="headerlink" title="Database 관련 생각해봐야 할것들"></a>Database 관련 생각해봐야 할것들</h4><ul>
<li>Production data에 대한 보호,보안(바꾸는 것은 리스크가 있다)</li>
<li>Test data 필요</li>
<li>환경이 다름</li>
<li>의존성</li>
<li>다수 어플리케이션이 동일 db에 접근한다.</li>
<li>이전버전과 호환성 유지</li>
<li>롤백을 어떻게 할지</li>
</ul>
<h4 id="DevOps-Values"><a href="#DevOps-Values" class="headerlink" title="DevOps Values"></a>DevOps Values</h4><p>CALMS</p>
<ul>
<li>Culture - 변화를 수용하라</li>
<li>Automation - CI/CD</li>
<li>Lean - 엔드유저가 생상하는 value에 초점을 맞추고, 배치사이즈를 작게하라</li>
<li>Measurement - 모든것을 측정하고 개선된것을 보여줘라</li>
<li>Sharing - 정보를 공유하고 협업하라</li>
</ul>
<h4 id="DevOps-Tools"><a href="#DevOps-Tools" class="headerlink" title="DevOps Tools"></a>DevOps Tools</h4><p><code>Configuration Automation / Management:</code> Puppet, Ansible, Chef, Salt<br><code>Continuous Integration:</code>Jenkins, CruiseControl, Capistrano<br><code>Monitoring:</code>Icinga (nagios), Zenoss, Sweet, Graphite<br><code>Containerization:</code> Docker, Rocket</p>
<h4 id="DevOps-관련용어"><a href="#DevOps-관련용어" class="headerlink" title="DevOps 관련용어"></a>DevOps 관련용어</h4><p>CI/CD</p>
<ul>
<li>Continuous Integration 그리고 Continuous Deployment의 약어</li>
</ul>
<p>Continuous integration</p>
<ul>
<li>최신코드에서 변경된것을 사용가능하도록 빌드한다.</li>
</ul>
<p>Continuous deployment</p>
<ul>
<li>가능한 빠르게 배포 단계를 거치고 빌드된 해당 패키지가 프로덕션 환경으로 전환되도록 한다.</li>
</ul>
<p>Continuous delivery</p>
<ul>
<li>Continuous integration + continuous deployment</li>
</ul>
<p>Delivery pipeline</p>
<ul>
<li>일련의 자동화 단계로써 CI/CD를 수행한다.</li>
</ul>
<h4 id="Continuous-Integration"><a href="#Continuous-Integration" class="headerlink" title="Continuous Integration"></a>Continuous Integration</h4><p>아래와 같은 단계를 빈번하게 수행한다.</p>
<p><code>1. Development</code></p>
<ul>
<li>배치 테스를 거친 작은 변화들이 빠르게 구현된다.</li>
</ul>
<p><code>2. SCM(Source Code management)</code></p>
<ul>
<li>여러 개발자의 변경된것사항을 병합한다.(GitHub, SVN … 활용)</li>
</ul>
<p><code>3. Build</code></p>
<ul>
<li>배포할것을 만든다.(Jenkins, Gradle, Maven …  활용)</li>
</ul>
<p><code>4. Package</code></p>
<ul>
<li>런타임때 빌드를 설치한다.</li>
<li>변경할 수 없는 이미지를 런타임때 releasing 한다.</li>
<li>클라우드에 푸쉬하고 컨테이너 이미지 빌드</li>
</ul>
<h4 id="Continuous-integration-–-package-step"><a href="#Continuous-integration-–-package-step" class="headerlink" title="Continuous integration – package step"></a>Continuous integration – package step</h4><p>package step에선 변경 불가능한 이미지가 만들어진다. 이는 deploying instance를 생성할 때 사용된다. 이미지를 변경하고 싶다면 삭제하고 새로 만들어야 한다.</p>
<h4 id="Continuous-Deployment"><a href="#Continuous-Deployment" class="headerlink" title="Continuous Deployment"></a>Continuous Deployment</h4><p>Production 배포까지의 과정<br><code>1. Deploy to Test</code></p>
<ul>
<li>functional testing 진행, test tool을 활용한 자동화<br><code>2. Deploy to Stage</code></li>
<li>Production 배포 전 리허설</li>
<li>통합 testing 진행<br><code>3. Deploy to Prod</code></li>
<li>사용자가 사용할 수 있도록 빌드</li>
</ul>
<h4 id="Continuous-Delivery-vs-continuous-Deployment"><a href="#Continuous-Delivery-vs-continuous-Deployment" class="headerlink" title="Continuous Delivery vs continuous Deployment"></a>Continuous Delivery vs continuous Deployment</h4><img src="/2019/06/08/msa4/1.PNG" alt="Continuous Delivery vs continuous Deployment" title="Continuous Delivery vs continuous Deployment">
<h4 id="Zero-Downtime-Deployment"><a href="#Zero-Downtime-Deployment" class="headerlink" title="Zero Downtime Deployment"></a>Zero Downtime Deployment</h4><p>서비스 중단없이 새로운 버전을 배포하는것을 의미한다. DevOps는 잦은 배포를 하므로 필요하다.</p>
<h4 id="Zero-Downtime-Deployment-특징"><a href="#Zero-Downtime-Deployment-특징" class="headerlink" title="Zero Downtime Deployment 특징"></a>Zero Downtime Deployment 특징</h4><ul>
<li>어플리케이션은 항상 사용가능하다.</li>
<li>사용자가 중단(Downtime)없이 사용할 수 있다.</li>
<li>이전버전, 새버전이 동시에 배포됨 - 트래픽이 둘 다로 전달됨.</li>
</ul>
<h4 id="Implementing-Zero-Downtime-Deployment-–-Blue-Green"><a href="#Implementing-Zero-Downtime-Deployment-–-Blue-Green" class="headerlink" title="Implementing Zero Downtime Deployment – Blue Green"></a>Implementing Zero Downtime Deployment – Blue Green</h4><p>Deployment<br>필요한경우 이전버전으로 신성하게 되돌릴 수 있다.</p>
<p>1.v1(blue)이 배포되어 사용자들이 사용하고 있을 때  v2(green)을 배포한다.<br>2.자동화된 테스트 도구로 테스트, 검증을 진행하고 사용자를 v1에서 v2환경으로 변환한다.<br>3.v1(blue)을 삭제한다.</p>
<h4 id="DevOps로의-Transformation"><a href="#DevOps로의-Transformation" class="headerlink" title="DevOps로의 Transformation"></a>DevOps로의 Transformation</h4><ul>
<li>Top to Bottom문화를 변화시키는 것이 핵심이다.</li>
<li>교육을 늘리고 커뮤니케이션하고 cross-skilling 하라</li>
<li>DevOps가 가능한 새로운 프로세스를 평가하라</li>
<li>스스로 재평가하고 재빌드하라</li>
<li>DevOps를 지원하는 새로운 기술 평가</li>
<li>조직을 작게 나눠라</li>
</ul>
<h4 id="Bottom-Up-Implementation"><a href="#Bottom-Up-Implementation" class="headerlink" title="Bottom-Up Implementation"></a>Bottom-Up Implementation</h4><ul>
<li>협업을 위한 방법을 찾아라. (사람들을 초기부터 참가시킨다.)</li>
<li>자동화 방법을 찾아라</li>
<li>metrics driven이 되어라</li>
<li>새로운것을 배우고 지속적으로 개선해라</li>
<li>작은 batches와 함께 병렬적으로 일해라</li>
<li>리팩토링을 허락해라</li>
<li>경영진에게 비즈니스 가치를 입증해라</li>
<li>비즈니스의 목표, metrics, 우선순위를 이해하라.</li>
</ul>
<h4 id="Top-Down-Implementation"><a href="#Top-Down-Implementation" class="headerlink" title="Top-Down Implementation"></a>Top-Down Implementation</h4><ul>
<li>시험 케이스를 파일럿으로 선택하라</li>
<li>우수 사레를 문서화하고 전파하라</li>
<li>팀의 역량을 강화하고 가치를 이끌어내라</li>
<li>측정 가능한 결과를 요구하라</li>
<li>과거의 baseline이 충분치 않을때 변명하지 마라</li>
<li>빠르게 실패하고 지속적으로 향상하라</li>
<li>작은성공을 기반으로 나아가라</li>
<li>비효율적인 경우 그룹간 역할 및 책임을 조율하라.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;DevOps&quot;&gt;&lt;a href=&quot;#DevOps&quot; class=&quot;headerlink&quot; title=&quot;DevOps&quot;&gt;&lt;/a&gt;DevOps&lt;/h3&gt;&lt;p&gt;Using DevOps to Solve your Delivery Challenges&lt;/p&gt;
&lt;ul
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/tags/MSA/"/>
    
  </entry>
  
  <entry>
    <title>MSA이론3. Database Design for Microservices</title>
    <link href="http://KKimSangHeon.github.io/2019/06/08/msa3/"/>
    <id>http://KKimSangHeon.github.io/2019/06/08/msa3/</id>
    <published>2019-06-08T05:18:22.000Z</published>
    <updated>2019-06-11T11:05:18.913Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Database-Design-for-Microservices"><a href="#Database-Design-for-Microservices" class="headerlink" title="Database Design for Microservices"></a>Database Design for Microservices</h3><p>트랜잭션, Aggregates에 영향을 많이끼친다.</p>
<p>Front Microservice는 쉽다.</p>
<h4 id="ACID가-적용되는-것들"><a href="#ACID가-적용되는-것들" class="headerlink" title="ACID가 적용되는 것들"></a>ACID가 적용되는 것들</h4><p>Transaction, Aggregates, Entity</p>
<h3 id="ACID-vs-BASE"><a href="#ACID-vs-BASE" class="headerlink" title="ACID vs BASE"></a>ACID vs BASE</h3><p>ACID</p>
<ul>
<li>Atomicity, Consistency, Isolation, and Durability</li>
<li>Strong consistency</li>
<li>commit에 초점을 맞춘다.</li>
<li>보수적이다.(혁신이 어렵다)</li>
</ul>
<p>BASE</p>
<ul>
<li>Basically Available, Soft-state, Eventual consistency</li>
<li>Weak consistency</li>
<li>Availability first</li>
<li>간단하고 빠르다(혁신이 쉽다)</li>
</ul>
<h4 id="구현을-위해-생각해보자"><a href="#구현을-위해-생각해보자" class="headerlink" title="구현을 위해 생각해보자"></a>구현을 위해 생각해보자</h4><ul>
<li>마이크로서비스에서 데이터는 dependency를 갖기 때문에 어려운부분이다.</li>
<li>microservice에 각각의 db가 있을 때 어떻게 읽고 어떻게 업데이트할까? join활용</li>
<li>data inside, 서비스, data outside 를 이해할 필요가있다.</li>
<li>데이터를 어떻게 읽고 업데이트하는가?</li>
<li>읽고 쓰는 명령 쿼리를 분할한다.(CQRS, Command Query Responsibility Segregation)</li>
</ul>
<h4 id="Two-phase-commit"><a href="#Two-phase-commit" class="headerlink" title="Two-phase commit"></a>Two-phase commit</h4><p>모든 트랜잭션은 트랜잭션 관리자를 거쳐야 한다. 이는 모든 서비스에서 트랜잭션을 보장하며 커밋 요청, 커밋 두단계의 메커니즘을 거친다.</p>
<p>커밋요청단계(커밋 투표단계라고도 함) : 관리자가 모든 서비스에 커밋메세지 쿼리를 보낸다. 관리자는 모든 서비스의 투표 결과를 대기하고 각 서버는 커밋해야 할 지점까지 트랜잭션을 수행한다. 서비스는 일부가 준비되지 않아도 동의한다는 응답을 보낸다.</p>
<p>커밋단계(완료단계라고도 함)</p>
<ul>
<li>관리자가 모든 서비스에서 동의를 받았을 때: 관리자는 트랜잭션 커밋을 요청하는 메세지를 모든 서비스에게 전송한다. 커밋 후에 서비스는 트랜잭션이 끝났다는 확인을 관리자에게 보낸다.</li>
<li>관리자가 하나 이상의 거절을 받았을 때 : 관리자는 모든 서비스들에게 롤백을 요청하고 서비스는 롤백 후 관리자에게 승인응답을 보낸다.</li>
</ul>
<h4 id="Two-phase-commit-의-장점"><a href="#Two-phase-commit-의-장점" class="headerlink" title="Two-phase commit 의 장점"></a>Two-phase commit 의 장점</h4><p>많은 DB가 영향을 받을경우 이를 하나의 트랜잭션으로 다루는것이 좋다.<br>준비단계는 모든 구성요소가 커밋을 허용하는지 확인하는 단계이다.</p>
<h4 id="Two-phase-commit-의-단점"><a href="#Two-phase-commit-의-단점" class="headerlink" title="Two-phase commit 의 단점"></a>Two-phase commit 의 단점</h4><ul>
<li>관리자는 서비스에서 메세지를 수신할 때 까지 잠금이 걸려 리소스가 해제되지 않음.(가용성 측면)</li>
<li>서비스 중 하나가 커밋을 거부하면 모든 서비스가 롤백한다.(일관성을 유지할 수 있다는 장점)</li>
<li>서비스에서 응답을 수신할 때 까지 잠금,리소스가 해제되지 않음</li>
<li>NoSQL을 지원하지 않음.</li>
<li>최소 O(4n) 메세지 발생, 여러번시도할 경우 O(n^2)</li>
<li>lock으로 인한 처리량 감소</li>
</ul>
<h4 id="Distributed-Transactions-in-Microservices"><a href="#Distributed-Transactions-in-Microservices" class="headerlink" title="Distributed Transactions in Microservices"></a>Distributed Transactions in Microservices</h4><img src="/2019/06/08/msa3/1.PNG" alt="Distributed Transactions in Microservices" title="Distributed Transactions in Microservices">
<p>microservice에는 ACID를 적용하기가 복잡하다. 이는 microservice Architecture가  Single Responsibility Principle (SRP)를 따르므로 각 microservice가 해당 데이터를 유지하고 관리해야하고, microservice의 책임에 대한 지식없이는 다른서비의 데이터에 접근할 수 없기때문이다.</p>
<p>위 그림에서 두개의 microservice가 하나의 db에 연결되어있는게 왜 잘못되었을까? 우측의 경우에는 API, Queue로 해당 호출이 가능하다. 하지만 좌측의 경우에는 두개의 db 아답터가 가 존재해야 하는데 Isolation 원칙을 위배한다.</p>
<h4 id="Distributed-Transaction-–-Event-Driven-Architecture-amp-Two-Phase-Commit"><a href="#Distributed-Transaction-–-Event-Driven-Architecture-amp-Two-Phase-Commit" class="headerlink" title="Distributed Transaction – Event Driven Architecture &amp; Two Phase Commit"></a>Distributed Transaction – Event Driven Architecture &amp; Two Phase Commit</h4>
<ul>
<li>이는 가장 일관된 시스템을 제공하지만 Two Phase Commit으로 인해 시스템 성능에 부정적 영향을 끼친다.</li>
<li>이벤트는 트랜잭션의 매 단계를 실행시키는 트리거이며 각 이벤트는 고유한 ID를 갖는다. 또한 작업이 처리되었으면 커밋되었음을 나타내야 한다.</li>
</ul>
<img src="/2019/06/08/msa3/3.PNG" alt="Distributed Transaction – Event Driven Architecture & Two Phase Commit2" title="Distributed Transaction – Event Driven Architecture & Two Phase Commit2">
<p>위의 그림은 microservice1이 microservice2에 자동이체를 하는 트랜잭션이다.</p>
<ul>
<li>completed라는 컬럼은 트랜잭션이 커밋되어야 하는지를 나타내며 microservice1이 완료되면 microservice2를 호출하여 트랜잭션을 완료한다.(그림은 microservice1의 작업이 커밋되고 microservice2의 작업이 커밋되기 전 단계)</li>
</ul>
<h4 id="CAP-Theorem"><a href="#CAP-Theorem" class="headerlink" title="CAP Theorem"></a>CAP Theorem</h4><img src="/2019/06/08/msa3/4.PNG" alt="CAP Theorem" title="CAP Theorem">
<p>분산시스템에서는 일관성(Consistency), 가용성(Availability,항상 데이터를 읽고 쓸 수 있음), 분할 용인(Partition tolerance,데이터베이스를 분할할 수 있고 네트워크 중단에도 계속 동작할 수 있다.)이라는 세 가지 조건을 모두 만족할 수 없다. 대부분 세 조건중 두 가지만 만족시킬 수 있다.<br>만약 가용성을 만족시키려면 BASE(Basically available, soft state, eventually consistent)를 생각해 볼 수 있다. 이를 위해 앞서 설명한 Event Driven Architecture를 도입할 수 있다.<br><code>BASE</code> : 기본적으로 Availability하고, 사용자가 관리하지 않으면 데이터가 expire 될 수 있으며, 언젠가는 데이터가 일관성을 가진다는 것.</p>
<h4 id="Eventual-Consistency-and-Compensation"><a href="#Eventual-Consistency-and-Compensation" class="headerlink" title="Eventual Consistency and Compensation"></a>Eventual Consistency and Compensation</h4><ul>
<li>microservice에서 일관성을 다루는 것중 가장 실현 가능한것은 Eventual Consistency이다.</li>
<li>microservice에서 분산 ACID 트랜잭션을 사용하지 않는다. 대신 미래의 어떤시점에서는 시스템이 결국 일관성을 유지할것을 제안한다.</li>
<li>Eventual Consistency 서비스는 종종 BASE를 제공하는것으로 분류된다.</li>
<li>Eventual Consistency는 분산 소프트웨어의 복잡성을 증가시킨다고 비판받기도 하는데 이는 동일값을 읽는다는 안전보장을 하지 않기 때문에 발생한다(결국에는 읽었을 때 동일값을 반환 함 예시로 배치프로그램이 있음).</li>
</ul>
<h4 id="Eventual-Consistency의-예시"><a href="#Eventual-Consistency의-예시" class="headerlink" title="Eventual Consistency의 예시"></a>Eventual Consistency의 예시</h4><p>다음의 문제를 해결해야 된다고 가정해보자.</p>
<ul>
<li>user profile 등록</li>
<li>백그라운드에서는 사용자가 시스템에 접근할 수 있는지 자동으로 확인</li>
</ul>
<p>Eventual Consistenc 적용을 하려면..</p>
<ul>
<li>user의 접근 가능유무는 반드시 필요하다.</li>
<li>user검증 시간이 오래 걸리더라도 microservice로 분할해야한다.</li>
</ul>
<p>compensation을 포함한 message-driven방법 적용</p>
<ul>
<li>user profile을 등록하는 작업</li>
<li>백그라운드에서 유효성을 검사하는 microservice</li>
<li>지속적인 큐를 제공하는 메세지 플랫폼<br>(메세지 플랫폼은 microservice가 보낸 메세지가 지속가능토록 한다. 수신부가 문제가 있다면 나중에 배달해주기도 함)<br>-이는 하나의 microservice가 중지될 경우 정보는 다른 큐, 서비스에 존재한다. 즉 지속성을 보장한다.</li>
</ul>
<p>위의 방법을 적용했을 때 좋은 시나리오</p>
<ul>
<li>user microservice는 user를 등록하고 로컬 db에 정보를 저장한다.</li>
<li>user microservice는 플래그를 활용해 해당 user가 검증 과정을 거치지 않음을 표시한다.</li>
<li>user에게 지금 당장은 접근 불가능하다는 경고를 보낸다.</li>
<li>user microservice는 백그라운드에서 user검증을 위해 validation microservice에 메세지를 보낸다.</li>
<li>검증결과 접근가능하다면 user microservice는 user를 unblock한다/ 검증결과 접근 불가능하다면 user microservice는 user 계정을 제거한다. (이를 compensation(보상)단계라고 볼 수 있다.)</li>
</ul>
<p>일련의 과정을 거친후에는 시스템은 일관된 상태이다. 하지만 일정기간동안은 사용자 엔티티가 불완전한 상태였다.</p>
<p>위의 방법을 적용했을 때 좋지 않은 시나리오</p>
<ul>
<li>validation microservice에 접근할수 없는경우 메세지 플랫폼은 나중에 validation microservice가 접근 가능하게 되었을 때 validation microservice는 메세지를 받아볼 수 있다.</li>
<li>메세지 플랫폼에 문제가 발생하면 user microservice는 또 다시 다른 user의 검증 메세지를 보낸다.</li>
<li>validation microservice가 메세지를 받게되면 user를 검증하지만 메세지 플랫폼 문제가 발생하면 응답을 보내지 못하고 추 후 다시 보낸다.</li>
</ul>
<p>그러나 일부메세지가 여러번 발행되더라도 데이터 일관성에는 영향을 미치지 않는다.<br>발생할 수 있는 좋지 않은 시나리오들을 고려함으로써 Eventual Consistency를 충족시킬 수 있으며 비용소모가 심한  distributed transactions를 처리할 필요가 없다.</p>
<h4 id="Distributed-Transactions-The-Solution"><a href="#Distributed-Transactions-The-Solution" class="headerlink" title="Distributed Transactions: The Solution"></a>Distributed Transactions: The Solution</h4><p>결제하고 배송하는 시스템을 Distributed Transaction방식으로 구현한다고 해보자<br><img src="/2019/06/08/msa3/5.PNG" alt="Distributed Transactions The Solution1" title="Distributed Transactions The Solution1"></p>
<p>위와 같은 방식으로 할 수 있으나 일련의 과정은 동기방식이다.<br>이를 비동기 방식으로 처리하기 위해 아래와 같은 방식을 따르면 된다. 즉 쓰레드를 하나 만들어서 처리한다.</p>
<img src="/2019/06/08/msa3/6.PNG" alt="Distributed Transactions The Solution1" title="Distributed Transactions The Solution1">
<p>안전한 Distributed Transactions를 위해서</p>
<ul>
<li>원격 업데이트에 대한 영향 감수</li>
<li>원격 업데이트를 시도하고 응답을 받지 못했을 경우 재시도를 해야한다. 또한 응답이 오더라도 자체 대기열을 업데이트하지 못할 경우 대기열이 사용가능해지면 다시 시도해서 업데이트 해야한다.</li>
<li>재시도로 발생하는 중복수신은한번만 처리한다</li>
<li>하나의 작업단위를 완료하더라도 시스템 오류가 발생하면 동기화 되지 않을 수 있다.</li>
<li>이를통해볼 때 microservice는 Distributed 트랜잭션에 호의적이지 않다.</li>
</ul>
<p>동기관련 문제를 해결할 수 있는 메세지 브로커<br><img src="/2019/06/08/msa3/7.PNG" alt="Distributed Transactions The Solution3" title="Distributed Transactions The Solution3"></p>
<h4 id="Two-Consequences-of-Eventual-Consistency"><a href="#Two-Consequences-of-Eventual-Consistency" class="headerlink" title="Two Consequences of Eventual Consistency"></a>Two Consequences of Eventual Consistency</h4><ul>
<li>재시도를 하는것은 시스템에 문제가 생긴것을 의미한다.</li>
<li>일관성이 결국에는 생기는것이기 때문에 비즈니스 일관성이 충돌하는 경우도 있을 수 있다.<br>만약 책을 구매하는 비즈니스가 있다고 할 때 결제를 하는순간 재고가 있어 결제가 진행되었는데 결제가 완료되는 순간 재고가 없을 때를 가정해보자. 이 때는 비동기식으로 사용자에게 다시 보고하지 않고 환불절차를 진행 후 직원이 처리 할 내용을 conflicts큐에 넣고 제공할 수 있다.</li>
</ul>
<h4 id="Saga-Pattern-microservice에서-어떻게-비즈니스-트랜잭션을-구현-할것인가"><a href="#Saga-Pattern-microservice에서-어떻게-비즈니스-트랜잭션을-구현-할것인가" class="headerlink" title="Saga Pattern : microservice에서 어떻게 비즈니스 트랜잭션을 구현 할것인가."></a>Saga Pattern : microservice에서 어떻게 비즈니스 트랜잭션을 구현 할것인가.</h4><p>트랜잭션은 어플리케이션의 필수적인 부분이다. 트랜잭션이 없다면 데이터의 일관성을 유지하는 것은 불가능하다.</p>
<p>가장 강력한 트랜잭션 유형중 하나는 Two-Phase Commit이다. 이는 여러 엔티티를 동시에 업데이트 할 때 유용하다.(ex. 주문확인 및 재고 업데이트)</p>
<p>그러나 microservice로 작업할 경우 데이터베이스가 분리되므로 로컬  Two-Phase Commit를 활용하여 전체 시스템의 일관성을 간단하게 유지할 수 없다. 이 경우 RDBMS와 마찬가지로 단일엔티티 원자 트랜잭션이 가능한 Couchbase와 같은 NoSQL 데이터베이스를 사용하면 수십배 빠르게 처리할 수 있다. 그래서 microservice를 사용하는 대다수 기업들이 NoSQL을 사용하고 있다.</p>
<h4 id="SAGA-Pattern"><a href="#SAGA-Pattern" class="headerlink" title="SAGA Pattern"></a>SAGA Pattern</h4><ul>
<li>분산 트랜잭션의 잘알려진 패턴중 하나가 SAGA이다.</li>
<li>SAGA는 일련의 local 트랙잭션들을 의미하며 각 트랜잭션은 하나의 서비스안에서 데이터를 업데이트 한다. 첫 번째 트랜잭션은 시스템 작업에 해당하는 외부 요청에 의해 시작되고 이후엔 이전단계 완료가 될 때마다 트리거링되어 작동한다.<br>-SAGA 트랜잭션을 구현하는 인기있는 두 가지 방법이 있다.<h4 id="1-Events-Choreography"><a href="#1-Events-Choreography" class="headerlink" title="1. Events/Choreography"></a>1. Events/Choreography</h4>각 서비스는 다른 서비스의 event, decides를 보고 action을 할지 말지 결정하며 non centralize 한것. (발레와 유사함)</li>
</ul>
<img src="/2019/06/08/msa3/EventsChoreography.PNG" alt="Events/Choreography" title="Events/Choreography">
<p>첫번째 서비스는 트랜잭션을 실행하고 이벤트를 publish한다. 발행된 이벤트는 하나 혹은 그 이상의 서비스가 지켜보며 해당 이벤트는 로컬 트랜잭션을 실행하고 새로운 이벤트를 publish한다.<br>분산 트랜잭션은 마지막에 서비스가 로컬 트랜잭션을 실행할 때 종료된되며 마지막 이벤트는 이벤트를 publish하지 않는다.<br>분산 트랜잭션의 경우 롤백에 대한 로직은 직접 만들어야 한다.</p>
<p>Event/Choreography design의 장단점</p>
<ul>
<li>이해하기 쉽고 SAGA패턴을 구현하는 자연스러운 방법이다. 구축에 많은 노력이 필요하지 않으며 느슨한 결합을 유지한다. 2~4단계로 구성되는 트랜잭션에 매우 적합하다.</li>
<li>어떤 서비스가 어떤 이벤트를 수신하는지 추적하기 어렵기 때문에 단계를 계속 추가할 경우 혼란스러울 수 있다.</li>
</ul>
<h4 id="2-Command-Orchestration-coordinator-서비스가-의사결정-및-sequncing-비즈니스-로직에-대한-책임이-있는-것-즉-centralize-한것-오케스트라와-유사함"><a href="#2-Command-Orchestration-coordinator-서비스가-의사결정-및-sequncing-비즈니스-로직에-대한-책임이-있는-것-즉-centralize-한것-오케스트라와-유사함" class="headerlink" title="2. Command/Orchestration : coordinator 서비스가 의사결정 및 sequncing 비즈니스 로직에 대한 책임이 있는 것. 즉 centralize 한것. (오케스트라와 유사함)"></a>2. Command/Orchestration : coordinator 서비스가 의사결정 및 sequncing 비즈니스 로직에 대한 책임이 있는 것. 즉 centralize 한것. (오케스트라와 유사함)</h4><img src="/2019/06/08/msa3/CommandOrchestration.PNG" alt="CommandOrchestration" title="CommandOrchestration">
<ul>
<li>Orchestration 접근법에서는 각 참가자에게 할 일을 알려줄 책임이 있는 새로운 서비스를 정의한다. 각 서비스와 명령/응답 형태로 통신하여 수행해야 할 작업을 알려준다.</li>
<li>위 그림의 경우 Orchestration은 트랜잭션을 실행하는데 필요한 흐름을 알고있다.</li>
<li>만약 트랜잭션이 실패하면 이전 작업을 취소하기 위해 각 참가자에게 롤백 명령을 보내야 한다. 롤백은 Orchestrator을 갖고있으면 훨씬 쉽다.</li>
<li>saga orchestrator을 모델링하는 표준 방법은 각 변환이 명령 또는 메세지에 해당하는지에 대한 State Machine이다. State Machine는 구현하기 쉽고 테스트에 적합하기 때문에 잘 정의된 동작을 구조화 하는데 훌륭한 패턴이다.</li>
</ul>
<p>Event/Choreography design의 장단점</p>
<ul>
<li>saga orchestrator는 saga participants를 호출할 수 있지만 saga participants는 saga orchestrator를 호출할수 없기에  서비스간 cyclic 종속성을 피할수 있다.</li>
<li>command에 대해 응답 혹은 실행만 하므로 participants간 복잡성을 줄일 수 있다.</li>
<li>새로운 단계가 추가될 때 트랜잭션의 복잡성은 linear하게 늘어난다.</li>
<li>쉬운 롤백관리 가능</li>
<li>첫번째,두번째 트랜잭션이 동일한 개체를 변경하고자 할 때 orchestrator를 활용하여 첫번째 트랜잭션이 끝날때 까지 두번째 트랜잭션을 보류상태로 둘 수 있다.</li>
<li>하지만 이는 orchestrator에 많은 로직이 들어가 risk가 있다</li>
<li>추가 서비스를 관리해야 하므로 인프라 복잡성이 증가한다.</li>
</ul>
<h4 id="Saga-Pattern-Tips"><a href="#Saga-Pattern-Tips" class="headerlink" title="Saga Pattern Tips"></a>Saga Pattern Tips</h4><p>트랜잭션마다 unique Id를 만들어라</p>
<ul>
<li>이를 통해 추적이 가능하고 participants간 통신을 위한 표준방법을 갖는데 도움을 준다.<br>command에 응답 address를 추가해라</li>
<li>participant가 고정된 주소에 응답하기 보다는 message에 응답 address를 추가하여 보내는것을 고려해봐라. 이를 통해 participants는 여러 orchestrator에 응답할 수 있다.</li>
<li>큐(SQS, Kafka, RabbitMQ, etc)를 사용할 경우 서비스간 통신할 수 있다.</li>
<li>버그로 인해 원치않은 메세지를 수신함으로써 데이터베이스가 엉망이 될 수 있다.<br>동기식 통신을 피하라</li>
<li>이를 통해 더 많은 데이터를 요청할 수 있고 다른 서비스가 오프라인 일 때도 서비스가 로컬 트랜잭션을 실행할 수 있다.</li>
<li>orchestrator는 각 요청/응답을 다뤄야하므로 선형적으로 복잡성이 증가한다.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Database-Design-for-Microservices&quot;&gt;&lt;a href=&quot;#Database-Design-for-Microservices&quot; class=&quot;headerlink&quot; title=&quot;Database Design for Micros
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/tags/MSA/"/>
    
  </entry>
  
  <entry>
    <title>MSA이론2. Microservice Architecturure</title>
    <link href="http://KKimSangHeon.github.io/2019/06/08/msa2/"/>
    <id>http://KKimSangHeon.github.io/2019/06/08/msa2/</id>
    <published>2019-06-08T05:18:16.000Z</published>
    <updated>2019-06-11T11:09:16.950Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Microservices-Architecture"><a href="#Microservices-Architecture" class="headerlink" title="Microservices  Architecture"></a>Microservices  Architecture</h3><ul>
<li>비즈니스 로직을 나누었다.</li>
<li>큰 프로그램 대신 몇몇의 작은 어플리케이션이다.</li>
<li>잘 정의된 API로 소통한다.(HTTP, AMQP 활용)</li>
<li>하나의 서비스가 정지될경우 다른것에 영향을 미치지 않는다</li>
</ul>
<h4 id="What-is-Microservice-Architecture"><a href="#What-is-Microservice-Architecture" class="headerlink" title="What is Microservice Architecture?"></a>What is Microservice Architecture?</h4><ul>
<li>각각의 컴포넌트를 시스템에 집어넣는것이며 각각 빌드되고 배포될수 있다.</li>
<li>Microservice 는 vertical 하며 layered하다. 또한 Process driven 형태이다.</li>
<li>Microservice를 잘 정의하면 TDD가 가능하다.</li>
<li>엔터프라이즈 아키텍처가 아니다. Microservice Architecture는 SOA와 유사하다.</li>
<li>단일 비즈니스 기능을 구현한다.</li>
<li>자신만의 데이터베이스를 갖는것이 일반적이나 데이터베이스를 갖지않는경우도 있다.</li>
<li>HTTP, AMQP 로 통신한다.</li>
<li>독립적으로 배포가 가능하다.</li>
<li>각각의 레파지토리를 갖는다.</li>
<li>마이크로 서비스 팀은 몇명이 적당할까? 피자 두판을 먹을 수 있을만한 인원수! 6명 ? 좋다!</li>
</ul>
<h4 id="Microservice의-구성"><a href="#Microservice의-구성" class="headerlink" title="Microservice의 구성"></a>Microservice의 구성</h4><ul>
<li>Data Store</li>
<li>Application/Logic</li>
<li>Public API(POST, GET )</li>
</ul>
<h4 id="Microservices-application의-전형적인-생태계"><a href="#Microservices-application의-전형적인-생태계" class="headerlink" title="Microservices application의 전형적인 생태계"></a>Microservices application의 전형적인 생태계</h4><p>원칙1. Microservices들은 각 public API에 서로 의존한다.</p>
<ul>
<li>백엔드의 마이크로 서비스는 노출되면 안된다. 오직 프론트 마이크로 서비스만 노출되어야 한다.</li>
</ul>
<p>원칙2. 작업에 적합한 tool(ex. 프레임워크)을 사용하라.</p>
<p>원칙3. 서비스 보안에 신경써라</p>
<p>원칙4. 생태계에서 좋은 시민이 되어라!</p>
<ul>
<li>모니터링, 로깅, 추적을 분산하라</li>
</ul>
<p>원칙5. 기술 변화 이상의 것이다.</p>
<ul>
<li>조직 변화를 수용하라</li>
</ul>
<p>원칙6. 모든것을 자동화하라(DevOps!)</p>
<h4 id="Micro-Service를-위해서…"><a href="#Micro-Service를-위해서…" class="headerlink" title="Micro Service를 위해서…"></a>Micro Service를 위해서…</h4><ul>
<li>비즈니스 도메인을 이해하자</li>
<li>일관성 유지하자</li>
<li>서비스 발견해보자</li>
<li>불필요한 상호통신이 많다면 조정해보자</li>
</ul>
<h4 id="Microservice-계획하기"><a href="#Microservice-계획하기" class="headerlink" title="Microservice 계획하기"></a>Microservice 계획하기</h4><ul>
<li>정말 옳은 선택인가?(트레이드 오프 고려)</li>
<li>시스템의 주요기능을 식별하자</li>
<li>서비스 컴포넌트의 스코프를 세부적으로 결정하라(Function의 크기, 타입, 복잡성)</li>
<li>API들을 디자인하라</li>
<li>커뮤니케이션의 메커니즘을 결정하라</li>
<li>데이터 모델을 결정하라(중앙 데이터베이스 vs 여러 데이터 저장소)</li>
</ul>
<h4 id="Benefits-of-microservices"><a href="#Benefits-of-microservices" class="headerlink" title="Benefits of microservices"></a>Benefits of microservices</h4><ul>
<li>각각의 microservices를 쉽개 확장할 수 있다</li>
<li>빠른 빌드, 테스트, 릴리즈의 싸이클</li>
<li>agility의 증가</li>
<li>빠른혁신이 가능하다.</li>
<li>명확한 소유권 그리고 책임의 분배.</li>
</ul>
<h4 id="Microservice-Architecture"><a href="#Microservice-Architecture" class="headerlink" title="Microservice Architecture"></a>Microservice Architecture</h4><img src="/2019/06/08/msa2/MicroserviceArchitecture.PNG" alt="Microservice Architectur" title="Microservice Architectur">
<ul>
<li>각 microservice는 컨테이너에 할당된다. 컨테이너는 microservice 기반 어플리케이션을 개별적으로 개발, 배포하기 좋은 방법이다.</li>
<li>microservice 간 stateless server 형태로 소통한다.</li>
<li>클라이언트는 서비스를 바로 호출할수 없고 API 게이트웨이가 클라이언트의 요청을 적절한 microservice로 전달한다.<br>(API 게이트웨이는microservice로인가요? 하드웨어나 소프트웨어로 볼 수 있다. microservice로 일수도 있지만 아닐수도 있다.)</li>
<li>각 서비스는 상호 독립적이다.</li>
<li>Single-responsibility 원칙을 따름</li>
<li>아무것도 공유하지 않는다.</li>
<li>비동기가 가능하다.</li>
<li>configuration의 외부화</li>
<li>결합도가 느슨하다.</li>
<li>하나의 서비스가 문제를 일으켜도 나머지는 상관없다.</li>
</ul>
<h4 id="Microservices-Disadvantages"><a href="#Microservices-Disadvantages" class="headerlink" title="Microservices - Disadvantages"></a>Microservices - Disadvantages</h4><p>Complex networking으로 인해 데이터베이스, 서버의 오버헤드 발생</p>
<h4 id="Monolithic-Architecture-vs-Microservice-Architecture"><a href="#Monolithic-Architecture-vs-Microservice-Architecture" class="headerlink" title="Monolithic Architecture vs Microservice Architecture"></a>Monolithic Architecture vs Microservice Architecture</h4><img src="/2019/06/08/msa2/MonolithicArchitecturevsMicroservice.PNG" alt="Monolithic vs Microservice" title="Monolithic vs Microservice">
<p><code>Monolithic Architecture</code> : 모든 기능이 단일 코드베이스에 위치하고 하나의 DB를 사용한다. 또한 한 기능이 마비되면 전체가 마비되며 크고 복잡한 어플리케이션형태</p>
<h4 id="Microservices-vs-SOA"><a href="#Microservices-vs-SOA" class="headerlink" title="Microservices vs SOA"></a>Microservices vs SOA</h4><img src="/2019/06/08/msa2/MicroservicesvsSOA.PNG" alt="Microservices vs SOA" title="Microservices vs SOA">
<p>SOA 컨센은 centrallize이다.<br>MS는 not centrallize이다.<br>즉 SOA는 오케스트라와 같다. (한 지위자가 전체를 통솔한다) Microservices는 발레와 같다. (각 댄서가 각자 율동한다)</p>
<h4 id="Monolithic-Architecture-vs-SOA-vs-Microservices"><a href="#Monolithic-Architecture-vs-SOA-vs-Microservices" class="headerlink" title="Monolithic Architecture vs SOA vs Microservices"></a>Monolithic Architecture vs SOA vs Microservices</h4><img src="/2019/06/08/msa2/MonolithicArchitecturevsSOAvsMicroservices.PNG" alt="Microservices vs SOA" title="Microservices vs SOA">
<p><code>coarse-grained</code> : 특정 프로세스(서비스)를 큰 덩어리로 나누는것<br><code>fine-grained</code> :  특정 프로세스(서비스)를 잘게 쪼개는 것</p>
<h3 id="Microservices를-구현해야-하는-9가지-이유"><a href="#Microservices를-구현해야-하는-9가지-이유" class="headerlink" title="Microservices를 구현해야 하는 9가지 이유"></a>Microservices를 구현해야 하는 9가지 이유</h3><ul>
<li>Easy To Build &amp; Maintain</li>
<li>Continuous Delivery</li>
<li>Hybrid Technologies</li>
<li>Cross Team Coordination</li>
<li>Higher Quality Code</li>
<li>Smarter Scaling</li>
<li>Risk Reduction</li>
<li>Promote Big Data Best Practices</li>
<li>Improved ROI with reduced TCO</li>
</ul>
<h4 id="Illustration-of-Monolithic-Module-Refactoring"><a href="#Illustration-of-Monolithic-Module-Refactoring" class="headerlink" title="Illustration of Monolithic Module Refactoring"></a>Illustration of Monolithic Module Refactoring</h4><img src="/2019/06/08/msa2/IllustrationofMonolithicModuleRefactoring.PNG" alt="IllustrationofMonolithicModuleRefactoring" title="IllustrationofMonolithicModuleRefactoring">
<p>Microservice Architecture는 3계층이다.Container/ Orchestration /Application</p>
<h4 id="The-twelve-Factors"><a href="#The-twelve-Factors" class="headerlink" title="The twelve Factors"></a>The twelve Factors</h4><p>I. Codebase: One codebase that is tracked in revision control, with many deployments<br>II. Dependencies: Explicitly declare and isolate dependencies<br>III.Configuration: Store Configuration in the environment<br>IV.Backing services: Treat backing services as attached resources<br>V. Build, release, run: Strictly separate build and run stages<br>VI.Processes: Execute the app as one or more stateless processes<br>VII.Port binding: Export services with port binding<br>VIII.Concurrency: Scale out using the process model<br>IX.Disposability: 빠른 시작 및 효율적인 종료가 가능하다.<br>X. Development and production parity: Keep development, staging, and production as similar as possible<br>XI.Logs: Treat logs as event streams<br>XII.Admin processes: Run administrative and management tasks as one-off processes</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Microservices-Architecture&quot;&gt;&lt;a href=&quot;#Microservices-Architecture&quot; class=&quot;headerlink&quot; title=&quot;Microservices  Architecture&quot;&gt;&lt;/a&gt;Microse
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/tags/MSA/"/>
    
  </entry>
  
  <entry>
    <title>MSA이론1. Domain-Driven Design / Aggregates</title>
    <link href="http://KKimSangHeon.github.io/2019/06/06/msa1/"/>
    <id>http://KKimSangHeon.github.io/2019/06/06/msa1/</id>
    <published>2019-06-06T10:26:59.000Z</published>
    <updated>2019-06-11T11:08:15.923Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Implementing-Domain-Driven-Design-For-Microservices-Architecture"><a href="#Implementing-Domain-Driven-Design-For-Microservices-Architecture" class="headerlink" title="Implementing Domain-Driven Design For Microservices Architecture"></a>Implementing Domain-Driven Design For Microservices Architecture</h3><h4 id="DDD의-원칙"><a href="#DDD의-원칙" class="headerlink" title="DDD의 원칙"></a>DDD의 원칙</h4><p><code>Values:</code> Meaning, Unity, Usability, Fitness, Flexibility, Maintainability<br><code>Principles:</code>  Continuous Learning, Knowledge Rich Design, Ubiquitous Language, Model-Driven Design, Separation of Concerns<br>Deep Models, Declarative Style<br><code>Patterns:</code><br>Layered Architecture<br>Ubiquitous Language (Entities, Value Objects, Services, Modules, Aggregates, Factories, Specification) /<br>Supple Design (Intention-Revealing Interfaces, Side-Effect Free Functions, Assertions, Conceptual Contours, Standalone Classes, Closure of Operations)</p>
<h3 id="What-is-Domain-Driven-Design"><a href="#What-is-Domain-Driven-Design" class="headerlink" title="What is Domain Driven Design"></a>What is Domain Driven Design</h3><p>도메인 전문가, 기술 전문가가 소프트웨어 개발를 위해 협엽하는 기술<br>아이디어와 도메인은 공통언어를 통해 코드에 반영되어야 한다.<br>Domain Driven Design 에 대해 알아보기 위해 다양한 Domain에 대해 알아보자.</p>
<h4 id="Domain"><a href="#Domain" class="headerlink" title="Domain?"></a>Domain?</h4><p>팀은 특정 비즈니스 도메인에 맞게 일한다.<br>팀은 비즈니스 도메인에 초점을 맞춘다<br>도메인의 세부사항은 팀의 포지션마다 다르다.</p>
<p>도메인의 예시<br>Hotel / Banking / Mortgage / Credit / Debit Accounts / Credit Cards / Retails loans</p>
<h4 id="Subdomains"><a href="#Subdomains" class="headerlink" title="Subdomains"></a>Subdomains</h4><ul>
<li>도메인은 서브도메인으로 구성된다.</li>
<li>서브도메인은 Bounded Context와 유사하며 이는 서로 커뮤니케이션할 수 있다.</li>
<li>서브도메인은 또다른 서버도메인을 포함할 수 있다.</li>
</ul>
<h4 id="Core-domain"><a href="#Core-domain" class="headerlink" title="Core domain"></a>Core domain</h4><ul>
<li>돈을 벌게해주는 중요한 도메인</li>
<li>경쟁업체와 높은 차별성을 갖음</li>
</ul>
<h4 id="Supporting-Subdomain"><a href="#Supporting-Subdomain" class="headerlink" title="Supporting Subdomain"></a>Supporting Subdomain</h4><p>기술적으로 서포팅하지만 COTS-Software가 아니다.<br>외부지원으로 구현될수 있지만 사내 팀이 주도해야 한다.</p>
<p><code>Commercial, off-the-shelf</code><br>COTS 란 완성품으로 일반 대중에게 판매, 대여 또는 권한을 부여할 수 있는 컴퓨터 소프트웨어나 하드웨어, 기술 또는 컴퓨터 제품 등을 의미한다.</p>
<h4 id="Generic-Subdomain"><a href="#Generic-Subdomain" class="headerlink" title="Generic Subdomain"></a>Generic Subdomain</h4><p>Suitable for Outsourcing, COTS</p>
<h4 id="Bounded-Contexts"><a href="#Bounded-Contexts" class="headerlink" title="Bounded Contexts"></a>Bounded Contexts</h4><p>큰 도메인을 작은 Context로 나눈것.<br>각각의 Context는 자신만의 공통언어, 모델을 갖을 수 있다.<br>또한 Bounded Contexts는 일부 도메인을 공유할 수 있다.<br>Ubiquitous Language(공통언어)로 모델되어야 하며 프로그램에서 비즈니스 니즈를 정의한다.</p>
<h3 id="This-is-Domain-Driven-Design"><a href="#This-is-Domain-Driven-Design" class="headerlink" title="This is Domain Driven Design"></a>This is Domain Driven Design</h3><h4 id="DDD의-Concepts-and-Overview"><a href="#DDD의-Concepts-and-Overview" class="headerlink" title="DDD의 Concepts and Overview"></a>DDD의 Concepts and Overview</h4><p>Domain-Driven Desigin은 <u>기본 비즈니스 이해에 중점을 둔 소프트웨어 디자인방식이다.</u><br>이러한 접근방식은 다음 두가지 전제를 둔다.</p>
<ul>
<li>복잡한 도메인 디자인은 모델을 기반으로 한다.</li>
<li>대부분의 소프트웨어 프로젝트는 도메인 및 도메인로직에 중점을 둔다. (시스템 구현을 위한 특정 기술에 중점을 두는것이 아님)</li>
</ul>
<h4 id="전통적인-Layered-Architecture"><a href="#전통적인-Layered-Architecture" class="headerlink" title="전통적인 Layered Architecture"></a>전통적인 Layered Architecture</h4><img src="/2019/06/06/msa1/1.PNG" alt="전통적인 Layered Architecture" title="전통적인 Layered Architecture">
<p><code>User Interface</code>: 정보를 보여주고 사용자의 명령을 해석<br><code>Application</code> : 비즈니스 rule,지식 미포함 / 작업 조율 그리고 도메인에 작업 위임역할<br><code>Domain</code> : 비즈니스로직, 룰 포함/ 소프트웨어의 심장부<br><code>Infrastructure</code> : 상위 레이어를 지원하는 기술제공</p>
<h4 id="Domain-Driven-Design-with-Onion-Architecture"><a href="#Domain-Driven-Design-with-Onion-Architecture" class="headerlink" title="Domain-Driven Design with Onion Architecture"></a>Domain-Driven Design with Onion Architecture</h4><img src="/2019/06/06/msa1/2.PNG" alt="Onion Architecture" title="Onion Architecture">
<p><code>Core</code>: 특정 도메인이나 기술에 국한되지 않는 building blocks로 볼 수 있다. 예로 List, Maps, Case Classes, Actor and Lenses가 있다.<br><code>Domain</code> : 공통언어를 통해 작성된 비즈니스 로직 관련 메소드, 클래스가 상주하는 영역이다.<br><code>API</code> :  도메인의 진입점 역할을 한다. API는 도메인을 조작하지 못하도록 immutable한 객체만 노출해야 한다. 코어, 도메인은 API에 액세스할 수 있지만 Infrastructure는 API에 액세스할 수 없다.<br><code>Infrastructure</code> : DB, 사용자 인터페이스 같은 다얗안 기술을 포함하는 가장 바깥쪽 영역이다. 모든 영역은 Infrastructure 영역에 접근 할 수 있다.</p>
<h4 id="Bounded-Contexts-에-대해-자세히-알아보자"><a href="#Bounded-Contexts-에-대해-자세히-알아보자" class="headerlink" title="Bounded Contexts 에 대해 자세히 알아보자"></a>Bounded Contexts 에 대해 자세히 알아보자</h4><p>전체 비즈니스 모델은 너무 커 한번에 이해하기 힘들다. Bounded Contexts는 서로 다른 모델간 경계와 관계를 표현하기 위해 존재하는 명시적인 경계로서 경계 내의 Ubiquitous Language는 특정한 의미를 갖는다.</p>
<img src="/2019/06/06/msa1/3.PNG" alt="Bounded Context" title="Bounded Context">
<h4 id="Bounded-Context의-구현"><a href="#Bounded-Context의-구현" class="headerlink" title="Bounded Context의 구현"></a>Bounded Context의 구현</h4><ul>
<li>Bounded Context당 한팀이 존재한다.</li>
<li>코드 Repository가 Bounded Context마다 존재한다.</li>
<li>Domain Model + DB Schema + UI + Web Services (API)로 구성된다.</li>
</ul>
<p><u>Bounded Context는 Ubiquitous Language와 domain model을 캡슐화하나 도메인 모델과 상호작용하는 것, 도메인 모델을 서포트 하는 기능을 포함한다.  </u></p>
<p>또한 Bounded Context안에  Aggregates Entity(유니크한 트랜잭션), Value Object(불변의 객체)가 존재하는데 이는 바로 뒤에서 설명하니 참고하도록 하자.</p>
<h4 id="What-are-Entities"><a href="#What-are-Entities" class="headerlink" title="What are Entities?"></a>What are Entities?</h4><p><u>Domain object를 엔티티로 정의하는데 그것은 제각각 다르다</u>. 또한 엔티티의 정체성을 적절히 결정하고 어떻게 가져올지 결정한다.<br>최종 사용자,각 어플리케이션,data store은 <u>identity</u>를 만들어낸다.</p>
<h3 id="Aggregates"><a href="#Aggregates" class="headerlink" title="Aggregates"></a>Aggregates</h3><p>Aggregates는 루트엔티티로 간주되는 연관된 객체 그룹이다. (트랜잭션과 같음)</p>
<h4 id="Aggregates의-특징"><a href="#Aggregates의-특징" class="headerlink" title="Aggregates의 특징"></a>Aggregates의 특징</h4><ul>
<li>트랜잭션과 마찬가지로 Atomic, Consistent, Isolated, Durable 특징을 갖는다.<br>(사람모형 레고는 2개의 팔, 다리, 얼굴을 갖는데 이를 Product’s Invariant(불변성)이라 한다.)</li>
<li>경계가 명확하다.(외부 개체는 신경쓰지 않음)</li>
<li>내부 개체를 보호한다. 외부개체는 루트 Aggregates를 통해 접근 가능하며 Aggregates의 상태는 변경 불가능하다.</li>
<li>Aggregates는 자신이 소유한 entity, value object들의 무결성을 보호해야할 책임이 있다.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">@Entity</div><div class="line">public class Cart implements Aggregate &#123;</div><div class="line">  @EmbeddedId</div><div class="line">  private CartId id;</div><div class="line">  @Embedded</div><div class="line">  private CustomerId customerId;</div><div class="line">  @OneToMany(cascade = CascadeType.All, orhanRemoval = true)</div><div class="line">  @JoinColumn(name=&quot;cartId&quot;)</div><div class="line">  private Set&lt;CartItem&gt; items;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>위와같은 코드가 있을 때 cart id는 루트 엔티티 이다. 또한 CartItem이라는 엔티티들의 레퍼런스를 갖고 있으며 CustomerId를 ValueObject로서 사용한다.</p>
<h4 id="Aggregates-rule"><a href="#Aggregates-rule" class="headerlink" title="Aggregates rule"></a>Aggregates rule</h4><ol>
<li>모델은 불변해야 하며 일관되게 경계안에 있어야 한다.</li>
<li>Aggregates 작게 디자인해라.(클 경우 확장성 저하 가능성)</li>
<li>다른 Aggregates참조는 Identity로 하라.</li>
<li>경계밖에서의 일관성유지</li>
</ol>
<h4 id="Relationships-Between-Aggregate"><a href="#Relationships-Between-Aggregate" class="headerlink" title="Relationships Between Aggregate"></a>Relationships Between Aggregate</h4><p>RelationshipsBetweenAggregate<br><img src="/2019/06/06/msa1/4.PNG" alt="Relationships Between Aggregate" title="Relationships Between Aggregate"></p>
<p>Aggregate Root만 다른 Bounded Context의 Aggregate Root에 접근 가능</p>
<h4 id="Aggregate-팁"><a href="#Aggregate-팁" class="headerlink" title="Aggregate 팁"></a>Aggregate 팁</h4><ul>
<li>Aggregate는 항상 정답이 아니다.</li>
<li>Aggregates는 루트와 연결될 수 있다.</li>
<li>루트가 아닌 엔티티를 FK로 사용하는것을 간과하지 마라.</li>
</ul>
<h4 id="Aggregates가-중요한-이유"><a href="#Aggregates가-중요한-이유" class="headerlink" title="Aggregates가 중요한 이유"></a>Aggregates가 중요한 이유</h4><p>개체를 그룹화하고 카테고리화 하면 복잡한것을 쉽게 관리할 수 있다.<br>주인없는 레코드를 방지하여 GC가 쉬워진다.<br>DB와의 고수준의 통신이 가능케한다.</p>
<h3 id="Value-Object"><a href="#Value-Object" class="headerlink" title="Value Object"></a>Value Object</h3><p>가능한 엔티티 대신 값 개체를 사용하여 모델을 작성해야 한다.<br>Value 인지 아닌지 결정하기위해 다음 것들을 확인해보자.</p>
<ul>
<li>도메인을 측정하고, 정량화 할수있는지</li>
<li>불변의 상태로 유지될 수 할수있는지</li>
<li>관련된 속성을 필수 단위로 하여 전체를 구성하는지</li>
<li>상황이 바뀌면 교체 가능한지.</li>
<li>Value를 사용하는 다른값과 비교될 수 할수있는지</li>
<li>collaborators에게 부작용을 없는 행동을 하는지</li>
</ul>
<h3 id="What-are-Domain-Services"><a href="#What-are-Domain-Services" class="headerlink" title="What are Domain Services"></a>What are Domain Services</h3><p>도메인의 일부는 객체로 모델링하는것이 자연스럽지 않다.<br>Application Service와 다르다. Application Service는 Domain Service의 클라이언트다.<br>일반적인 사용 예</p>
<ul>
<li>성능이 중요한 비즈니스 프로세스</li>
<li>도메인 객체를 다른것의 구성요소로 변환할 때</li>
<li>둘 이상의 도메인 객체에서 입력을 요구할 때</li>
</ul>
<h3 id="Domain-Building-Blocks"><a href="#Domain-Building-Blocks" class="headerlink" title="Domain Building Blocks"></a>Domain Building Blocks</h3><p>Entity :</p>
<ul>
<li>identity가 있는 명사</li>
<li>가변적이며 다른 엔티티 혹은 value object와 연관될 수 있다.</li>
<li>공유될 수 없다.</li>
</ul>
<p>Value Object :</p>
<ul>
<li>identity가 없는 명사</li>
<li>불변하며 다른 엔티티와 연관될 수 있다..</li>
<li>공유될 수 있다.</li>
</ul>
<p>Aggregate :</p>
<ul>
<li>하나의 Aggregate당 하나의 root entity가 있다.</li>
<li>관련있는 Aggregate는 루트 entity를 통해 참조할 수 있지만 Aggregate의 다른 entity는 참조할 수 없다.</li>
<li>모든 작업은 루트를 통해 수행된다.</li>
</ul>
<p>Service :</p>
<ul>
<li>서비스는 어플리케이션에서의 액션이다.</li>
<li>서비스는 엔티티의 상태변화를 일으킨다</li>
<li>서비스는 상태가 없다.</li>
<li>서비스는 어플리케이션,도메인, 인프라스트럭쳐의 어느 곳의 일부가 될 수 있다.</li>
</ul>
<p>Factory :</p>
<ul>
<li>엔티티나 value object를 생성한다</li>
<li>엔티티의 생성이 복잡할 때 사용한다.</li>
</ul>
<h4 id="DDD의-이점"><a href="#DDD의-이점" class="headerlink" title="DDD의 이점"></a>DDD의 이점</h4><ul>
<li>기술보다는 비즈니스에 초점을 맞춘다,</li>
<li>코드르 재사용하고 읽기 쉽다.</li>
<li>개선사항이 있을 때 유연하다.</li>
</ul>
<h4 id="성공적인-DDD"><a href="#성공적인-DDD" class="headerlink" title="성공적인 DDD"></a>성공적인 DDD</h4><ul>
<li>도메인전문가, 기술 전문가의 협업을 통해 모델 빌드</li>
<li>어플리케이션의 반복적인 빌드</li>
<li>테스트하고 테스트하고 또 테스트하라</li>
</ul>
<h4 id="세계지도에서-DDD가-어떻게-적용되는지-보자"><a href="#세계지도에서-DDD가-어떻게-적용되는지-보자" class="headerlink" title="세계지도에서 DDD가 어떻게 적용되는지 보자."></a>세계지도에서 DDD가 어떻게 적용되는지 보자.</h4><p>Model Driven Desigin<br>Domain = Word Map<br>Sub domain = 오세아니아, 아시아, 북아메리카,….<br>Bounded Context = Countries(South Korea)<br>Ubiquitous Language = Korean Language<br>Domain Model = Map of Korea</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Implementing-Domain-Driven-Design-For-Microservices-Architecture&quot;&gt;&lt;a href=&quot;#Implementing-Domain-Driven-Design-For-Microservices-Arch
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/tags/MSA/"/>
    
  </entry>
  
  <entry>
    <title>13. 헬름</title>
    <link href="http://KKimSangHeon.github.io/2019/06/06/kube13/"/>
    <id>http://KKimSangHeon.github.io/2019/06/06/kube13/</id>
    <published>2019-06-06T08:17:59.000Z</published>
    <updated>2019-06-11T10:58:31.407Z</updated>
    
    <content type="html"><![CDATA[<p>하나 이상의 클러스터를 운영하다 보면 같은 어플리케이션을 여러 클러스터에 배포해야 하는 경우가 발생한다.<br>이럴 때 배포 환경에 따라 달라지는 설정값들때문에 문제점들이 많이 발생한다.</p>
<p>그래서 배포 환경에 따라 달라지는 설정값만 정의해 둔 다음 이에 따라 배포하는 메커니즘이 필요했는데 이를 해결한것이 바로 헬름이다.<br>헬름은 쿠버네티스 차트를 관리하기 위한 도구이다. 차트는 사전 구성된 쿠버네티스 리소스의 패키지다. 즉 헬름은 패키지 관리도구이며, 차트가 리소스를 하나로 묶은 패키지에 해당한다.</p>
<ol>
<li>헬름 : 차트를 관리</li>
<li>차트(매니패스트 템플릿으로 구성) : 차트를 사용하여 매니페스트 파일 생성</li>
<li>매니페스트 파일 : 매니페스트 파일에 기초한 쿠버네티스 리소스 관리</li>
<li>쿠버네티스</li>
</ol>
<p>헬름으로 차트를 관리하는 목적은 번잡해지기 쉬운 매니페스트 파일을 관리하기 쉽게하기 위한것이다. 헬름은 단순한 패키지 관리자가 아니라, 차트를 중심으로 하는 쿠버네티스 개발 업무의 종합 관리도구이다.</p>
<p>실무에서는 여러 환경에 배포해야 하는 어플리케이션은 모두 차트로 패키징해 kubectl 대신 헬름으로 배포 및 없데이트를 수행한다.</p>
<h3 id="헬름-설치"><a href="#헬름-설치" class="headerlink" title="헬름 설치"></a>헬름 설치</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ curl -LO https://git.io/get_helm.sh</div><div class="line">$ chmod 700 get_helm.sh</div><div class="line">$ ./get_helm.sh</div></pre></td></tr></table></figure>
<p>헬름 초기화<br>틸러라는 서버 어플리케이션이 kube-system 네임스페이스에 배포된다. 틸러는 helm 명령에따라 설치 등의 작업을 담당한다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ helm init</div></pre></td></tr></table></figure>
<p>잘 만들어졌나 확인해보자<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl -n kube-system get service,deployment,pod --selector app=helm</div></pre></td></tr></table></figure></p>
<p>버전을 확인해보자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ helm version</div></pre></td></tr></table></figure></p>
<p>여러 클러스터를 다룰 때는 클라이언트/서버의 버전을 일치시키는것이 좋다. 이를 위해 다음을 입력하여 업그레이드하자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ helm init --upgrade</div></pre></td></tr></table></figure></p>
<h3 id="헬름"><a href="#헬름" class="headerlink" title="헬름?"></a>헬름?</h3><p>헬름은 클라이언트(cli)와 서버(쿠버네티스 클러스터에 설치되는 틸러)로 구성된다. 클라이언트는 서버를 대상으로 명령을 지시하고 서버는 클라이언트에서 전달받은 명령에 따라 쿠버네티스 클러스터에 패키지 설치, 업데이트, 삭제 등의 작업을 수행한다.</p>
<p>쿠버네티스는 서비스나 디플로이먼트, 인그레스 같은 리소스를 생성하고 매니페스트 파일을 적용하는 방식으로 어플리케이션을 배포한다. 이 매니패스트 파일을 생성하는 템플릿을 여러 개 패키징한것이 차트이다. 차트는 헬름 레파지토리에 tgz 파일로 저장되며 틸러가 매니페스트를 생성하는데 사용한다.</p>
<h3 id="리포지토리"><a href="#리포지토리" class="headerlink" title="리포지토리"></a>리포지토리</h3><p>헬름의 리포지토리는 다음과 같은 종류가 있다.</p>
<p><code>local</code> : 헬름 클라이언트가 설치된 로컬 리포지토리. 로컬에서 생성한 패키지가 존재<br><code>stable</code> : 안정버전의 차트가 존재하는 리포지토리.<br><code>incubator</code> : 아직 stable 하지는 못하지만 곧 stable로 넘어갈 예정인 차트</p>
<p>stable 리포지토리는 기본값으로 사용되며 깃허브 github.com/helm/charts에 저장된다.(궁금하면 참고해보자)</p>
<h3 id="차트의-구성"><a href="#차트의-구성" class="headerlink" title="차트의 구성"></a>차트의 구성</h3><p>차트는 다음과 같은 디렉터리 구성을 갖는다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">chart_name/ --- templates/ 매니페스트 파일 템플릿 디렉터리</div><div class="line">  | |- xxxx.yaml           각종 쿠버네티스 리소스의 매니페스트 템플릿</div><div class="line">  | |-_helper_tpl          매니페스트 랜더링에 사용되는 템플릿 헬퍼</div><div class="line">  | |-NOTE.txt             차트 사용법 등의 참조 문서 템플릿</div><div class="line">  |</div><div class="line">  |-chart/                 이 차트가 의존한느 차트의 디렉토리</div><div class="line">  |-Chart.yaml             차트 정보가 정의된 파일</div><div class="line">  |-values.yaml            차트 기본값 value 파일</div></pre></td></tr></table></figure></p>
<p>차트는 어플리케이션의 동작을 제어하는 설정의 기본값을 values.yaml 파일에 정의한다.<br><img src="/2019/06/06/kube13/jenkins.PNG" alt="차트의 디렉터리 예시" title="차트의 디렉터리 예시"></p>
<p>차트를 이용해 어플리케이션을 설치하려면 helm install 명령을 사용한다. 어플리케이션을 업데이트,삭제하기 위해서는 릴리즈 네임이 필요한데 –name 옵션으로 붙여준다. 이는 해당 클러스터 안에서 유일한 값이어야 한다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ helm install [--name 릴리스_네임] 차트_리포지토리/차트명</div></pre></td></tr></table></figure>
<p>helm install 명령을 실행하면 차트에 포함된 기본값 value 파일에 정의된 설정값으로 어플리케이션이 설치된다.그러나 기본값 value파일을 사용하는 경우는 드물며 일부 기본값을 수정한 커스텀 value 파일을 주로 사용한다. 이를 위해 각 리포지토리에서 제공하는 참고문서를 참고하자.</p>
<h3 id="레드마인-설치-예제"><a href="#레드마인-설치-예제" class="headerlink" title="레드마인 설치 예제"></a>레드마인 설치 예제</h3><p>문서에 규정된 설정값에 따라 커스텀 value 파일을 작성한다. 이를 redmine.yaml파일로 정의한다. 내용은 다음과 같다.<br>yaml파일에는 기본값에서 수정할 항목만 포함하면 된다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">redmineUsername: sangheon</div><div class="line">redminePassword: sangheon</div><div class="line">redmineLanguage: en</div><div class="line"></div><div class="line">serviceType: NodePort</div></pre></td></tr></table></figure></p>
<p>-f 옵션으로 커스텀 value 파일을 지정해서 레드마인을 설치한다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">$ helm install -f redmine.yaml --name redmine stable/redmine --version 4.0.0</div></pre></td></tr></table></figure></p>
<h3 id="에러발생시"><a href="#에러발생시" class="headerlink" title="에러발생시"></a>에러발생시</h3><p><code>namespaces &quot;default&quot; is forbidden: User &quot;system:serviceaccount:kube-system:default&quot; cannot get resource &quot;namespaces&quot; in API group &quot;&quot; in the namespace &quot;default&quot;</code></p>
<p>helm list라고 입력했을때<br><code>Error: configmaps is forbidden: User &quot;system:serviceaccount:kube-system:default&quot; cannot list resource &quot;configmaps&quot; in API group &quot;&quot; in the namespace &quot;kube-system&quot;</code> 라는 에러 발생시 해결방법</p>
<p>rbac-config.yaml 파일로 다음과 같이 생성한다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ServiceAccount</div><div class="line">metadata:</div><div class="line">  name: tiller</div><div class="line">  namespace: kube-system</div><div class="line">---</div><div class="line">apiVersion: rbac.authorization.k8s.io/v1</div><div class="line">kind: ClusterRoleBinding</div><div class="line">metadata:</div><div class="line">  name: tiller</div><div class="line">roleRef:</div><div class="line">  apiGroup: rbac.authorization.k8s.io</div><div class="line">  kind: ClusterRole</div><div class="line">  name: cluster-admin</div><div class="line">subjects:</div><div class="line">  - kind: ServiceAccount</div><div class="line">    name: tiller</div><div class="line">    namespace: kube-system</div></pre></td></tr></table></figure></p>
<p>다음을 순서대로 입력<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kubectl create -f rbac-config.yaml</div><div class="line">$ helm init --service-account tiller --history-max 200 --upgrade</div></pre></td></tr></table></figure></p>
<p>다시 다음을 입력하여 설치하면 될것이다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ helm install -f redmine.yaml --name redmine stable/redmine --version 4.0.0</div></pre></td></tr></table></figure></p>
<p>다음을 입력해서 설치한것을 확인할 수 있다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ helm ls</div><div class="line">$ kubectl get service,deployment --selector release=redmine</div></pre></td></tr></table></figure>
<h3 id="설치한것을-다시-삭제해보자"><a href="#설치한것을-다시-삭제해보자" class="headerlink" title="설치한것을 다시 삭제해보자"></a>설치한것을 다시 삭제해보자</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ helm delete redmine</div></pre></td></tr></table></figure>
<h3 id="헬름에서-제공하는-롤백기능-사용해보자"><a href="#헬름에서-제공하는-롤백기능-사용해보자" class="headerlink" title="헬름에서 제공하는 롤백기능 사용해보자"></a>헬름에서 제공하는 롤백기능 사용해보자</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ helm ls --all</div><div class="line">$ helm rollback redmine 1 #리비전 숫자 입력하기</div></pre></td></tr></table></figure>
<h3 id="리비전-기록을-남기지-않고-제거하기"><a href="#리비전-기록을-남기지-않고-제거하기" class="headerlink" title="리비전 기록을 남기지 않고 제거하기"></a>리비전 기록을 남기지 않고 제거하기</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ helm del --purge redmine</div></pre></td></tr></table></figure>
<h3 id="RBAC를-지원하는-어플리케이션-설치"><a href="#RBAC를-지원하는-어플리케이션-설치" class="headerlink" title="RBAC를 지원하는 어플리케이션 설치"></a>RBAC를 지원하는 어플리케이션 설치</h3><p>공개된 차트 중에는 RBAC를 활성화할 수 있는 어플리케션도 있다. RBAC를 활성화한 어플리케이션을 설치하려면 실제 설치 작업을 수행할 틸러에 cluster-admin이라는 롤(ClusterRole)이 부여돼야 한다. 다음과 같이 cluster-admin 롤을 갖는 서비스 계정을 생성한 다음, 이 계정을 실행중인 틸러의 서비스 계정으로 설정한다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ kubectl create serviceaccount --namespace kube-system tiller</div><div class="line">$ kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</div><div class="line">$ kubectl patch deploy --namespace kube-system tiller-deploy -p &apos;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&apos;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;하나 이상의 클러스터를 운영하다 보면 같은 어플리케이션을 여러 클러스터에 배포해야 하는 경우가 발생한다.&lt;br&gt;이럴 때 배포 환경에 따라 달라지는 설정값들때문에 문제점들이 많이 발생한다.&lt;/p&gt;
&lt;p&gt;그래서 배포 환경에 따라 달라지는 설정값만 정
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>12. 사용자 관리와 RBAC(role-based access control)</title>
    <link href="http://KKimSangHeon.github.io/2019/06/06/kube12/"/>
    <id>http://KKimSangHeon.github.io/2019/06/06/kube12/</id>
    <published>2019-06-06T08:17:55.000Z</published>
    <updated>2019-06-09T07:34:17.107Z</updated>
    
    <content type="html"><![CDATA[<p>쿠버네티스 사용자마다 권한을 제어하는 것은 쿠버네티스 운영하는데 있어 보안을 확보하는 기본적인 방법이다.</p>
<p>쿠버네티스 사용자는 두 가지 개념으로 나뉜다.<br><code>일반사용자</code>: 클러스터 외부에서 쿠버네티스를 조작하는 사용자로 다양한 방법으로 인증을 거친다. 개발자 및 운영 실무자가 쿠버네티스를 조작하기 위해 사용하며 쿠버네티스 클러스터 외부로부터 들어오는 접근을 관리하기 위한 사용자이다.<br>배포와 관련된 서비스나 디플로이먼트의 접근 권한을 일부 사용자에게만 허용하거나 파드의 로그 열람 권한을 다른 일반 사용자에게도 허용하는 등의 정책을 일반 사용자 권한 부여로 실현할 수있다.</p>
<p><code>서비스 계정</code>: 쿠버네티스 내부적으로 관리되며 파드가 쿠버네티스 API를 다룰 때 사용하는 사용자.(kubectl또한 쿠버네티스 API와 통신해 작동함) 주어진 권한에 따라 쿠버네티스 리소스(파드, 디플로이먼트 등)를 다룰 수 있다.<br>서비스 계정은 어플리케이션을 통해 쿠버네티스 조작을 통제할 수 있다는 점이 장점이다. 클러스터 안에서 봇을 동작시키는 파드에 권한을 부여해두고, 이 봇으로 기존 디플로이먼트를 업데이트하거나 레플리카 수를 조절하는 식으로 활용할 수 있다.</p>
<p>서비스 계정 및 일반 사용자의 권한은 RBAC(role-based access control) 라는 메커니즘을 통해 제어된다. RBAC는 롤에 따라 리소스에 대한 권한을 제어하는 기능이자 개념이다.</p>
<p>Role, Cluster Role은 접근가능한것에 대한 범위이고 binding은 롤을 주는것을 의미한다고 판단.</p>
<p><code>롤</code> : 각 쿠버네티스 API의 사용권한을 정의. 네임스페이스 안에서만 유효<br><code>롤바인딩</code> : 일반 사용자 및 그룹/서비스 계정과 롤을 연결<br><code>클러스터롤</code>: 각 쿠버네티스 API의 사용 권한을 정의. 클러스터 전체에서 유효<br><code>클러스터롤바인딩</code>: 일반사용자 및 그룹/서비스 계정과 클러스터롤을 연결</p>
<p>262p부터</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;쿠버네티스 사용자마다 권한을 제어하는 것은 쿠버네티스 운영하는데 있어 보안을 확보하는 기본적인 방법이다.&lt;/p&gt;
&lt;p&gt;쿠버네티스 사용자는 두 가지 개념으로 나뉜다.&lt;br&gt;&lt;code&gt;일반사용자&lt;/code&gt;: 클러스터 외부에서 쿠버네티스를 조작하는 
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>11. 쿠버네티스 실전편(잡, 크론잡, 시크릿)</title>
    <link href="http://KKimSangHeon.github.io/2019/06/02/kube11/"/>
    <id>http://KKimSangHeon.github.io/2019/06/02/kube11/</id>
    <published>2019-06-02T08:11:52.000Z</published>
    <updated>2019-06-06T08:27:16.197Z</updated>
    
    <content type="html"><![CDATA[<p>파드, 레플리카세트, 디플로이먼트, 서비스, 인그레스는 데몬으로 동작하는 서버 어플리케이션을 구축할 때 사용되는 기본 리소스이다<br>쿠버네티스는 데몬으로 동작하는 서버 어플리케이션 외에도 배치 서버등 다양한 형태의 어플리케이션을 구축할 수 있다.</p>
<h3 id="잡"><a href="#잡" class="headerlink" title="잡"></a>잡</h3><p>잡은 하나 이상의 파드를 생성해 지정된 수의 파드가 정상 종료될 때까지 이를 관리하는 리소스다.<br>잡이 생성한 파드는 정상 종료된 후에도 삭제되지 않고 그대로 남아있기 때문에 작업이 종료된 후에 파드의 로그나 실행 결과를 분석할 수 있다. 그러므로 배치작업 위추의 어플리케이션에 적합하다.<br>잡은 파드 여러개를 병렬로 실행하는 방법으로 쉽게 스케일 아웃이 가능하다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">apiVersion: batch/v1</div><div class="line">kind: Job</div><div class="line">metadata:</div><div class="line">  name: pingpong</div><div class="line">  labels:</div><div class="line">    app: pingpong</div><div class="line">spec:</div><div class="line">  parallelism: 3    # 동시에 실행하는 파드의 수를 지정하는 속성. 파드를 병렬로 실행해야할 때 편리</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: pingpong</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: pingpong</div><div class="line">        image: gihyodocker/alpine:bash</div><div class="line">        command: [&quot;/bin/sh&quot;]</div><div class="line">        args:</div><div class="line">          - &quot;-c&quot;</div><div class="line">          - |</div><div class="line">            echo [`date`] ping!</div><div class="line">            sleep 10</div><div class="line">            echo [`date`] pong!</div><div class="line">      restartPolicy: Never</div></pre></td></tr></table></figure>
<p>위의 코드를 yaml파일로 만들고 아래 명령어를 통해 확인해보자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kubectl apply -f test.yaml</div><div class="line">$ kubectl get pod -l app=pingpong --show-all</div></pre></td></tr></table></figure></p>
<p>parallelism와 replicas의 차이점은 무엇일까?<br>replicas가 3이면 동일한 파드를 3개 만들라는 것이고, parallelism이 이면 동시에 3개의 파드를 실행하는것인데… 확인해보니 parallelism도 파드를 3개 만들긴함.</p>
<p>잡 리소스는 restartPolicy 속성을 Never, OnFailure중 하나를 설정해야 한다.</p>
<h3 id="크론잡"><a href="#크론잡" class="headerlink" title="크론잡"></a>크론잡</h3><p>잡 리소스는 파드가 단 한번만 실행되는데 반해 크론잡 리소스는 스케줄을 지정해 정기적으로 파드를 실행할 수 있다. 즉 정기적으로 파드를 실행할 수 있다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">apiVersion: batch/v1beta1</div><div class="line">kind: CronJob</div><div class="line">metadata:</div><div class="line">  name: pingpong</div><div class="line">spec:</div><div class="line">  schedule: &quot;*/1 * * * *&quot;</div><div class="line">  jobTemplate:</div><div class="line">    spec:</div><div class="line">      template:</div><div class="line">        metadata:</div><div class="line">          labels:</div><div class="line">            app: pingpong</div><div class="line">        spec:</div><div class="line">          containers:</div><div class="line">          - name: pingpong</div><div class="line">            image: gihyodocker/alpine:bash</div><div class="line">            command: [&quot;/bin/sh&quot;]</div><div class="line">            args:</div><div class="line">              - &quot;-c&quot;</div><div class="line">              - |</div><div class="line">                echo [`date`] ping!</div><div class="line">                sleep 10</div><div class="line">                echo [`date`] pong!</div><div class="line">          restartPolicy: OnFailure</div></pre></td></tr></table></figure></p>
<p>spec.schedule속성에 Cron과 같은 포맷으로 파드를 실행할 스케줄을 정의한다. 또한 spec.jobTemplate 아래에 잡 리소스와 마찬가지로 파드 정의가 들어가면 된다.<br>보통의 경우 리눅스 crontab으로 스케줄에 맞춰 스크립트를 실행하는게 대부분이었따. 하지만 크론잡 리소스를 이용하면 이 모든것을 컨테이너로 해결할 수 있다.</p>
<h3 id="시크릿"><a href="#시크릿" class="headerlink" title="시크릿"></a>시크릿</h3><p>쿠버네티스의 시크릿 리소스를 사용하면 기밀정보 문자열을 Base 인코딩으로 만들 수 있다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Secret</div><div class="line">metadata:</div><div class="line">  name: nginx-secret</div><div class="line">type: Opaque</div><div class="line">data:</div><div class="line">  .htpasswd: eW91cl91c2VybmFtZTpyejc5SXpTalplaWZvCg==</div></pre></td></tr></table></figure>
<p>위 코드를 활용해 아래에 적용가능하다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: basic-auth</div><div class="line">spec:</div><div class="line">  type: NodePort</div><div class="line">  selector:</div><div class="line">    app: basic-auth</div><div class="line">  ports:</div><div class="line">  - protocol: TCP</div><div class="line">    port: 80</div><div class="line">    targetPort: http</div><div class="line">    nodePort: 30060</div><div class="line"></div><div class="line">---</div><div class="line">apiVersion: apps/v1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: basic-auth</div><div class="line">  labels:</div><div class="line">    app: basic-auth</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: basic-auth</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: basic-auth</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: nginx</div><div class="line">        image: &quot;gihyodocker/nginx:latest&quot;</div><div class="line">        imagePullPolicy: Always</div><div class="line">        ports:</div><div class="line">          - name: http</div><div class="line">            containerPort: 80</div><div class="line">        env:</div><div class="line">          - name: BACKEND_HOST</div><div class="line">            value: &quot;localhost:8080&quot;</div><div class="line">          - name: BASIC_AUTH_FILE</div><div class="line">            value: &quot;/etc/nginx/secret/.htpasswd&quot;    </div><div class="line">        volumeMounts:</div><div class="line">          - mountPath: /etc/nginx/secret    # 이 경로에 .htpasswd가 생성된다.</div><div class="line">            name: nginx-secret</div><div class="line">            readOnly: true</div><div class="line">      - name: echo</div><div class="line">        image: &quot;gihyodocker/echo:latest&quot;</div><div class="line">        imagePullPolicy: Always</div><div class="line">        ports:</div><div class="line">          - containerPort: 8080</div><div class="line">        env:</div><div class="line">          - name: HTTP_PORT</div><div class="line">            value: &quot;8080&quot;</div><div class="line">      volumes:</div><div class="line">      - name: nginx-secret</div><div class="line">        secret:</div><div class="line">          secretName: nginx-secret</div></pre></td></tr></table></figure>
<p>인증정보를 환경 변수로 관리하는 기법또한 존재한다. 이는 259p를 참고하자.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;파드, 레플리카세트, 디플로이먼트, 서비스, 인그레스는 데몬으로 동작하는 서버 어플리케이션을 구축할 때 사용되는 기본 리소스이다&lt;br&gt;쿠버네티스는 데몬으로 동작하는 서버 어플리케이션 외에도 배치 서버등 다양한 형태의 어플리케이션을 구축할 수 있다
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>10. 쿠버네티스의 스토리지</title>
    <link href="http://KKimSangHeon.github.io/2019/06/01/kube10/"/>
    <id>http://KKimSangHeon.github.io/2019/06/01/kube10/</id>
    <published>2019-06-01T04:17:08.000Z</published>
    <updated>2019-06-02T08:11:02.042Z</updated>
    
    <content type="html"><![CDATA[<p>마스터 슬레이브 형태로 MySQL을 구성하자.</p>
<h3 id="쿠버네티스의-스토리지"><a href="#쿠버네티스의-스토리지" class="headerlink" title="쿠버네티스의 스토리지"></a>쿠버네티스의 스토리지</h3><p>쿠버네티스에서는 호스트에서 분리할 수 있는 외부 스토리지를 볼륨으로 사용할 수 있다. 파드가 다른 호스트로 재배치 되어도 외부 스토리지 형태의 볼륨은 새로 배치된 호스트에 자동으로 할당된다. 그러므로 호스트와 데이터 볼륨의 결합이 느슨해지고 외부 스토리지를 사용하므로 퍼시스턴스 데이터를 다루는 애플리케이션을 컨테이너로 운영하기가 쉽다.<br>쿠버네티스에서 관련 리소스는 다음의 요소들이 있다.</p>
<ul>
<li>퍼시스턴트볼륨</li>
<li>퍼시스턴트볼륨클레임</li>
<li>스토리지클래스</li>
<li>스테이트풀세트</li>
</ul>
<h3 id="퍼시스턴트볼륨과-퍼시트턴트볼륨클레임"><a href="#퍼시스턴트볼륨과-퍼시트턴트볼륨클레임" class="headerlink" title="퍼시스턴트볼륨과 퍼시트턴트볼륨클레임"></a>퍼시스턴트볼륨과 퍼시트턴트볼륨클레임</h3><p>퍼시스턴트볼륨은 스토리지 자체이며 퍼시스턴트볼륨클레임은 추상화된 논리 리소스로 퍼시스턴볼륨과 달리 용량을 필요한 만큼 동적으로 확보 할 수 있는것이다.<br>퍼시스턴트볼륨클레임은 클러스터가 구축된 플랫폼을 지원하는 퍼시스턴스 볼륨을 생성하기 위해 사용된다.</p>
<p>다음은 퍼시스턴스볼륨클레임 리소스의 매니페스트 파일이다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: PersitentVolumeClaim</div><div class="line">metadata:</div><div class="line">  name: pvc-example</div><div class="line">spec:</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  storageClassName: ssd</div><div class="line">  resource:</div><div class="line">    requests:</div><div class="line">      storage: 4Gi</div></pre></td></tr></table></figure></p>
<p>위의 <code>accessModes</code>는 파드가 스토리지에 접근한는 방식을 지정한다. ReadWriteOnce 는 마운트 될 수 있는 노드를 하나로 제한한다는 의미이다. 이 외에도 ReadOnlyMany 혹은 ReadWriteMany가 있다. 이들은 이러한 제약이 없으며 플랫폼에 따라 사용할 수 없는 경우가 있으므로 주의하<br><code>storageClassName</code>는 StorageClass리소스의 종류 즉 어떤 스토리지를 사용할지를 정의한다.</p>
<h3 id="스토리지클래스-StorageClass"><a href="#스토리지클래스-StorageClass" class="headerlink" title="스토리지클래스(StorageClass)"></a>스토리지클래스(StorageClass)</h3><p>스토리지클래스는 퍼시스턴트볼륨으로 확보한 스토리지 종류를 정의하는 리소스다. 앞에서의 storageClassName속성값의 실체가 이것이다. GCP의 경우 storageClassName의 종류는 표준,SSD가 있다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">kind: StorageClass</div><div class="line">apiVersion: storage.k8s.io/v1</div><div class="line">metadata:</div><div class="line">  name: ssd</div><div class="line">  annotations:</div><div class="line">    storageclass.kubernetes.io/is-default-class: &quot;false&quot;</div><div class="line">  labels:</div><div class="line">    kubernetes.io/cluster-service: &quot;true&quot;</div><div class="line">provisioner: kubernetes.io/gce-pd</div><div class="line">parameters:</div><div class="line">  type: pd-ssd</div></pre></td></tr></table></figure>
<p>SSD 스토리지를 사용하도록 스토리지 클래스의  name 속성을 ssd로 하고 provisioner는 GCP 의 퍼시스턴스 스토리지인 GCEPersistentDisk에 해당하는 gcd-pd로 지정한다. 그리고 파라미터의 type 속성값을 pd-ssd로 지정한다.</p>
<h3 id="스테이트풀세트-StatefulSet"><a href="#스테이트풀세트-StatefulSet" class="headerlink" title="스테이트풀세트(StatefulSet)"></a>스테이트풀세트(StatefulSet)</h3><p>디플로이먼트는 함께 포함된 파드 정의를 따라 파드를 생성하는 리소스로 하나만 있으면 되는 파드 혹은 퍼시스턴스 데이터를 갖지않는 즉 상태가 없는(stateless) 어플리케이션을 배포하는데 적합하다.<br><u>이에 비해 스테이트풀 세트는 데이터 스토어처럼 데이터를 계속 유지하는 상태가 있는 애플리케이션을 관리하는데 적합한 리소스다.</u></p>
<p>디플로이먼트에서 생성한 파드는 무작위로 생성된 식별자가 부여된다. <u>스테이트풀세트는 pod-1, pod-2, pod-2와 같이 일련번호가 붙는 유일한 식별자를 붙여 파드를 생성한다.</u><br>이 식별자는 파드를 재생성해도 유지되며, 스케일링 할 때도 식별자의 일련번호가 계속 이어진다.</p>
<p>파드가 재생성되어도 스토리지가 계속 같은 파드에 연결되어 파드의 데이터를 그대로 복원할 수 있다.</p>
<p>/</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;마스터 슬레이브 형태로 MySQL을 구성하자.&lt;/p&gt;
&lt;h3 id=&quot;쿠버네티스의-스토리지&quot;&gt;&lt;a href=&quot;#쿠버네티스의-스토리지&quot; class=&quot;headerlink&quot; title=&quot;쿠버네티스의 스토리지&quot;&gt;&lt;/a&gt;쿠버네티스의 스토리지&lt;/h3&gt;&lt;p&gt;쿠
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>9. GCP 를 활용한 실습환경 구축</title>
    <link href="http://KKimSangHeon.github.io/2019/06/01/kube9/"/>
    <id>http://KKimSangHeon.github.io/2019/06/01/kube9/</id>
    <published>2019-06-01T04:15:34.000Z</published>
    <updated>2019-06-09T08:31:04.737Z</updated>
    
    <content type="html"><![CDATA[<p>온프레미스 환경 또는 퍼블릭 클라우드에서 쿠버네티스를 실제로 사용해 보자.<br>클라우드에서 Google Kubernetes Engine을이용하거나 온프레미스 환경에서 Kuberspray를 이용해 클러스터를 구축하자</p>
<h3 id="윈도우-OS의-경우"><a href="#윈도우-OS의-경우" class="headerlink" title="윈도우 OS의 경우"></a>윈도우 OS의 경우</h3><p>GCP를 생성하고 구글 클라우드 SDK를 설치하자</p>
<p>아래의 링크에서 GCP 설치<br><a href="https://cloud.google.com/sdk/docs/quickstart-windows?hl=ko" target="_blank" rel="external">https://cloud.google.com/sdk/docs/quickstart-windows?hl=ko</a></p>
<p>환경변수에 다음 추가<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">C:\Users\SangHeon\AppData\Local\Google\Cloud SDK\google-cloud-sdk\bin</div></pre></td></tr></table></figure></p>
<p>gcloud 버전 업데이트<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ gcloud components update</div></pre></td></tr></table></figure></p>
<p>아래 입력 후 계정입력<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ gcloud auth login</div></pre></td></tr></table></figure></p>
<p>아래를 입력하여 대상 프로젝트 ID 선택<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ gcloud config set project xxxx</div></pre></td></tr></table></figure></p>
<p>아래 입력 하여 리전 설정<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ gcloud config set compute/zone asia-northeast1-a</div></pre></td></tr></table></figure></p>
<h3 id="CentOS일-때"><a href="#CentOS일-때" class="headerlink" title="CentOS일 때"></a>CentOS일 때</h3><p>Centos 환경에서의 진행</p>
<p>아래를 복붙하자<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">sudo tee -a /etc/yum.repos.d/google-cloud-sdk.repo &lt;&lt; EOM</div><div class="line">[google-cloud-sdk]</div><div class="line">name=Google Cloud SDK</div><div class="line">baseurl=https://packages.cloud.google.com/yum/repos/cloud-sdk-el7-x86_64</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">repo_gpgcheck=1</div><div class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg</div><div class="line">       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</div><div class="line">EOM</div></pre></td></tr></table></figure></p>
<p>아래 명령어 입력<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># sudo yum install google-cloud-sdk</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># gcloud init</div></pre></td></tr></table></figure>
<p>나오는 url로 들어가 코드 복사 및 붙여넣기</p>
<p>아래 입력 하여 리전 설정<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ gcloud config set compute/zone asia-northeast1-a</div></pre></td></tr></table></figure></p>
<h3 id="쿠버네티스-클러스터를-생성해보자"><a href="#쿠버네티스-클러스터를-생성해보자" class="headerlink" title="쿠버네티스 클러스터를 생성해보자"></a>쿠버네티스 클러스터를 생성해보자</h3><p> <a href="https://console.cloud.google.com/apis/api/container.googleapis.com/overview?project=xxxxxx" target="_blank" rel="external">https://console.cloud.google.com/apis/api/container.googleapis.com/overview?project=xxxxxx</a><br> 으로 접속해서 사용 설정가능토록 해주자</p>
<p>아래 명령어로 클러스터 생성<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ gcloud container clusters create sangheon --cluster-version=latest --machine-type=n1-standard-1 --num-nodes=3</div></pre></td></tr></table></figure></p>
<p>–cluster-version 으로 클러스터 버전 지정<br>–num-nodes  으로 인스턴수 수 지정</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ gcloud components install kubectl #센토스의 경우 안될경우 10줄짜리를 다시 복붙해보고 sudo yum update kubectl 로</div></pre></td></tr></table></figure>
<p>kubectl 에 인증정보 설정<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ gcloud container clusters get-credentials sangheon</div></pre></td></tr></table></figure></p>
<p>잘되었나 확인해보자<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl get nodes</div></pre></td></tr></table></figure></p>
<p>다음을 입력하여 로컬에서 접속해보자<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl proxy</div></pre></td></tr></table></figure></p>
<hr>
<p>/</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;온프레미스 환경 또는 퍼블릭 클라우드에서 쿠버네티스를 실제로 사용해 보자.&lt;br&gt;클라우드에서 Google Kubernetes Engine을이용하거나 온프레미스 환경에서 Kuberspray를 이용해 클러스터를 구축하자&lt;/p&gt;
&lt;h3 id=&quot;윈도우-
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>8. 인그레스</title>
    <link href="http://KKimSangHeon.github.io/2019/05/29/kube8/"/>
    <id>http://KKimSangHeon.github.io/2019/05/29/kube8/</id>
    <published>2019-05-29T10:23:51.000Z</published>
    <updated>2019-05-29T12:56:24.086Z</updated>
    
    <content type="html"><![CDATA[<p>NodePort의 경우 L4레벨까지 다룰수 있기 때문에 HTTP/HTTPS처럼 경로를 기반으로 서비스를 전환하는 L7레벨의 제어는 불가능하다.<br>이를 해결하기 위한것이 인그레스이다. 서비스를 이용한 쿠버네티스 외부에 대한 노출(NodePort)과 가상 호스트 및 경로 기반의 정교한 HTTP 라우팅(인그레스)을 양립시킬 수 있다.<br>(무슨말인지 이해가 되지 않는다면 아래 밑줄부분을 보자.)</p>
<p>클러스터 외부에서 온 HTTP 요청을 서비스로 라우팅 하기위해 다음을 입력하자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.16.2/deploy/mandatory.yaml</div><div class="line">$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.16.2/deploy/provider/cloud-generic.yaml</div><div class="line"></div><div class="line">$ kubectl get pod,svc -n ingress-nginx</div></pre></td></tr></table></figure></p>
<p>서비스를 다음과 같이 생성하고 반영시켜보자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: echo</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    app: echo</div><div class="line">  ports:</div><div class="line">    - name: http</div><div class="line">      port: 80</div></pre></td></tr></table></figure></p>
<p>다음과같이 인그레스를 정의하고 반영해보자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: Ingress</div><div class="line">metadata:</div><div class="line">  name: echo</div><div class="line">spec:</div><div class="line">  rules:</div><div class="line">  - host: ch05.gihyo.local</div><div class="line">    http:</div><div class="line">      paths:</div><div class="line">      - path: /</div><div class="line">        backend:</div><div class="line">          serviceName: echo</div><div class="line">          servicePort: 80</div></pre></td></tr></table></figure></p>
<p>잘 생성되었는지 확인해보자.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl get ingress</div></pre></td></tr></table></figure></p>
<p><u>인그레스는 L7 라우팅이 가능하므로 가상 호스팅 기능처럼 지정된 호스트 혹은 경로와 일치하는 서비스로 요청을 전달할 수 있다</u> 가령 헤더에 Mobile라는 값이 포함될경우 특정 URL로 리다이렉트 할 수 있다.</p>
<h3 id="apiVersion"><a href="#apiVersion" class="headerlink" title="apiVersion??"></a>apiVersion??</h3><p>항상 맨위에 위치하는 apiVersion은 무엇일까?<br>쿠버네티스 리소스를 생성, 수정, 삭제하는 작업은 쿠버네티스 클러스터에 배포된 API가 수행한다. 이 API는 여러 API를 하나로 묶은 형태로 구성되는데 이 apiVersion은 해당 작업에 사용되는 API의 종류를 나타내는 것이다. 다음명령어를 통해 쿠버네티스에서 사용할 수 있는 명령어들을 볼 수 있다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl api-versions</div></pre></td></tr></table></figure></p>
<p>서비스나 파드는 쿠버네티스의 핵심 API인 v1, 디플로이먼트는 파드의 생성을 제어하는 API인 apps/v1에 해당한다.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NodePort의 경우 L4레벨까지 다룰수 있기 때문에 HTTP/HTTPS처럼 경로를 기반으로 서비스를 전환하는 L7레벨의 제어는 불가능하다.&lt;br&gt;이를 해결하기 위한것이 인그레스이다. 서비스를 이용한 쿠버네티스 외부에 대한 노출(NodePort
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>7. 서비스</title>
    <link href="http://KKimSangHeon.github.io/2019/05/29/kube7/"/>
    <id>http://KKimSangHeon.github.io/2019/05/29/kube7/</id>
    <published>2019-05-29T10:23:39.000Z</published>
    <updated>2019-05-29T10:24:38.043Z</updated>
    
    <content type="html"><![CDATA[<p>쿠버네티스 클러스터 안에서 파드의 집합(주로 레플리카세트)에 대한 경로나 서비스 디스커버리(API 주소가 동적으로 바뀌어도 클라이언트가 대상을 바꾸지않고 접근할 수 있음)를 제공하는 리소스이다. 서비스의 대상이 되는 파드는 서비스에서 정의하는 레이블 셀렉터로 정해진다. spec.selector 속성값으로 특정 파트의 레이블값을 설정하여 특정 파드만 접근할 수 있도록 yaml파일을 작성해보자.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: echo</div><div class="line">spec:</div><div class="line">  selector:     # 특정 파드만 접근할수 있도록 함.</div><div class="line">    app: echo</div><div class="line">    release: summer</div><div class="line">  ports:</div><div class="line">    - name: http</div><div class="line">      port: 80</div></pre></td></tr></table></figure>
<p>위의 서비스를 실행하고 컨테이너 안에 존재하는 아무 컨테이너에 들어가서 <code>curl http://echo</code> 를 입력하면 잘 동작하는것을 확인할 수 있다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ kubectl exec -it 파드명 bash</div><div class="line"></div><div class="line">혹은</div><div class="line"></div><div class="line">$ kubectl run -i --rm --tty debug --image=gihyodocker/fundamental:0.1.0 --restart=Never -- bash -il</div><div class="line"></div><div class="line"></div><div class="line"># curl http://echo</div></pre></td></tr></table></figure></p>
<p>다음명령어를 통해 로그를 summer안에서 생기는 로그를 확인할 수 있다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubeclt logs -f echo-summer-dtblk -c echo</div></pre></td></tr></table></figure></p>
<p>쿠버네티스 클러스터는 <code>서비스명.네임스페이스명.svc.local</code>로 연결해준다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ curl http://echo.default.svc.local</div><div class="line"></div><div class="line">$ curl http://echo.default</div><div class="line"># svc.local 생략가능</div><div class="line"></div><div class="line">$ curl http://echo  </div><div class="line"># 같은네임스페이스일 경우만 가능</div></pre></td></tr></table></figure>
<h3 id="ClusterIP-서비스"><a href="#ClusterIP-서비스" class="headerlink" title="ClusterIP 서비스"></a>ClusterIP 서비스</h3><p>서비스의 종류는 여러가지가 있고 기본값은 ClusterIP 서비스이다. ClusterIP를 통해 클러스터 내부 IP 주소에 서비스를 공개할 수 있다.</p>
<h3 id="NodePort-서비스"><a href="#NodePort-서비스" class="headerlink" title="NodePort 서비스"></a>NodePort 서비스</h3><p>이는 클러스터 외부에서 접근할 수 있는 서비스이다. ClusterIP를 만든다는 점은 ClusterIP 서비스와 같다. 각 노드에서 서비스 포트로 접속하기 위한 글로벌 포트를 개방하는것이 차이점이다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: echo</div><div class="line">spec:</div><div class="line">  type: NodePort  # 추가함</div><div class="line">  selector:     </div><div class="line">    app: echo</div><div class="line">    release: summer</div><div class="line">  ports:</div><div class="line">    - name: http</div><div class="line">      port: 80</div></pre></td></tr></table></figure>
<p>다음 명령어를 통해 포트를 알아내고 curl요청을 보낼 수 있다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl get svc echo</div></pre></td></tr></table></figure></p>
<h3 id="LoadBalancer-서비스"><a href="#LoadBalancer-서비스" class="headerlink" title="LoadBalancer 서비스"></a>LoadBalancer 서비스</h3><p>이는 각 클라우드 플랫폼에서 제공하는 로드밸런서와 연동하기 위해 사용된다.</p>
<h3 id="ExternalName-서비스"><a href="#ExternalName-서비스" class="headerlink" title="ExternalName 서비스"></a>ExternalName 서비스</h3><p>이는 셀렉터도 포트 정의도 없는 특이한 서비스다. 쿠버네티스 클러스터에서 외부 호스트를 네임 레졸루션 하기위한 별칭을 제공한다.<br>아래의 경우 gihyo.jp를 gihyo로 참조할 수 있게해준다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: gihyo</div><div class="line">spec:</div><div class="line">  type: ExternalName  # 추가함</div><div class="line">  externalName: gihyo.jp</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;쿠버네티스 클러스터 안에서 파드의 집합(주로 레플리카세트)에 대한 경로나 서비스 디스커버리(API 주소가 동적으로 바뀌어도 클라이언트가 대상을 바꾸지않고 접근할 수 있음)를 제공하는 리소스이다. 서비스의 대상이 되는 파드는 서비스에서 정의하는 레
    
    </summary>
    
      <category term="CS" scheme="http://KKimSangHeon.github.io/categories/CS/"/>
    
      <category term="MSA" scheme="http://KKimSangHeon.github.io/categories/CS/MSA/"/>
    
    
      <category term="kubernetes" scheme="http://KKimSangHeon.github.io/tags/kubernetes/"/>
    
  </entry>
  
</feed>
